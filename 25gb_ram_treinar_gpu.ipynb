{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "25gb_ram_treinar_gpu",
      "provenance": [],
      "collapsed_sections": [
        "dioJyoTnxEm3",
        "U6L7JmKxleYS",
        "HQ4bXO4HN9ow",
        "5uUN-aE_08kW",
        "L6HHuwUX_xDC",
        "U4pj8f8S_zut",
        "M9w4v5WxSjkN",
        "JwQSQSBT5-6H",
        "yw3Kv5xt6iDh",
        "kd8wHUp08Ry2",
        "yJv3Pp_kiAiA",
        "JsfaQKp2oF1N",
        "2uWF1axWdwc9",
        "j5A8onSqdwc_",
        "OL4Je_GLdwc_",
        "UvrkzoO8PgZ-",
        "3AbPa6gxvuIh",
        "ZkjruqQ_uVVS",
        "Ir7PEjV_RVhf",
        "c8NMyC8nDQLH",
        "G0cOVQD1D9RH",
        "dIHrYNpwO_wy",
        "5XtEvkw6PJG4",
        "FZY1Xs2QSXr3",
        "Cw5aUTJ3-L9w",
        "ngj2YXGh-yoa",
        "cRjI9wmXKB43",
        "jXAWW7JUS_J3",
        "Tbyo0BT5CHvF"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicholasrichers/dissertacao/blob/master/25gb_ram_treinar_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuvKj30nz1Ox"
      },
      "source": [
        "# RUNNING San_Francisco"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dioJyoTnxEm3"
      },
      "source": [
        "\n",
        "## 1. Install GPU Libs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5SwdgLRuolY"
      },
      "source": [
        "GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckB0vQUOuncj",
        "outputId": "1cee292b-d2c1-40fc-e916-c44f2f6e8014"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jan 22 07:07:27 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OuoXN-rum99",
        "outputId": "3efe1b85-3f78-4741-91c7-63cc32417079"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyRvqnLEVoIw"
      },
      "source": [
        "Light GBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swoKzjnrVoIx"
      },
      "source": [
        "%%capture\n",
        "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "\n",
        "%cd /content/LightGBM\n",
        "!mkdir build\n",
        "\n",
        "!cmake -DUSE_GPU=1\n",
        "!make -j$(nproc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1zG5nTbVoI1"
      },
      "source": [
        "%%capture\n",
        "!sudo apt-get -y install python-pip\n",
        "!sudo -H pip install setuptools numpy scipy scikit-learn -U\n",
        "\n",
        "%cd /content/LightGBM/python-package\n",
        "!sudo python setup.py install --precompile\n",
        "\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI7ThCLZV0F1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKT3Rc5dVoI9"
      },
      "source": [
        "XGboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjZBJJi7VoI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42492a14-a084-402d-beaa-c0b6e679ac66"
      },
      "source": [
        "#%%capture\n",
        "!pip install xgboost==1.2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xgboost==1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/5c/1133b5b8f4f2fa740ff27abdd35b8e79ce6e1f8d6480a07e9bce1cdafea2/xgboost-1.2.0-py3-none-manylinux2010_x86_64.whl (148.9MB)\n",
            "\u001b[K     |████████████████████████████████| 148.9MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost==1.2.0) (1.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost==1.2.0) (1.19.5)\n",
            "Installing collected packages: xgboost\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ToHxUFjquo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1dcm9wrPjZ9"
      },
      "source": [
        "Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PuNiQFTOpxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586a592d-8bae-4dc8-a630-b3bf38d8deaa"
      },
      "source": [
        "#%%capture\n",
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==0.23.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling scikit-learn-0.24.1:\n",
            "  Successfully uninstalled scikit-learn-0.24.1\n",
            "Collecting scikit-learn==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/3a/eb8d7bbe28f4787d140bb9df685b7d5bf6115c0e2a969def4027144e98b6/scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.1) (1.5.4)\n",
            "Installing collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-0.23.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXi0_oI4jsIt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6L7JmKxleYS"
      },
      "source": [
        "## 1.1 Quick Setup dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r3_8MelOXxr",
        "outputId": "f48cdf6c-b5ce-4343-e612-87d7befd981e"
      },
      "source": [
        "#checando versoes para evitar conflitos\n",
        "\n",
        "import lightgbm as lgbm\n",
        "print(\"lgbm version:\", lgbm.__version__)\n",
        "\n",
        "import xgboost as xgb\n",
        "print(\"xgb version:\", xgb.__version__)\n",
        "\n",
        "import sklearn\n",
        "print(\"sklearn version:\", sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lgbm version: 3.1.1.99\n",
            "xgb version: 1.2.0\n",
            "sklearn version: 0.23.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0gCHg9fks89"
      },
      "source": [
        "Git Clone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGycE6KbtZhi"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/nicholasrichers/dissertacao.git\n",
        "!pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GlQ5oFqAOin"
      },
      "source": [
        "Git Pull"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KNk_4eGAR4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75224c47-ddad-4fae-9c22-ee858e46e325"
      },
      "source": [
        "%cd /content/dissertacao/\n",
        "!git pull\n",
        "%cd /content\n",
        "#get_libs()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/dissertacao\n",
            "Already up to date.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ4bXO4HN9ow"
      },
      "source": [
        "## 1.2 Import libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbtrwdlnjgHh"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def load_library(NAME_LIB, FILE_PATH):\n",
        "    from importlib.machinery import SourceFileLoader\n",
        "    FILE_PATH = '/content/dissertacao/' + FILE_PATH\n",
        "    somemodule = SourceFileLoader(NAME_LIB, FILE_PATH).load_module()\n",
        "    \n",
        "\n",
        "load_library('quick_setup', 'src/utils/quick_setup.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV1kBU5zthQW"
      },
      "source": [
        "#import pandas as pd\n",
        "from quick_setup import get_libs\n",
        "get_libs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRTDxSQMnv6W"
      },
      "source": [
        "#private modules \n",
        "#data\n",
        "import make_dataset\n",
        "\n",
        "#models\n",
        "import train_model\n",
        "import evaluation\n",
        "import dsr\n",
        "import meta_model\n",
        "import neutralize\n",
        "fn_strategy = neutralize.fn_strategy_dict\n",
        "\n",
        "#validation\n",
        "import group_ts_split\n",
        "import combinatorial_split\n",
        "import metrics\n",
        "import metrics_era\n",
        "import metrics_description\n",
        "#import my_validation\n",
        "\n",
        "#visualization\n",
        "import visualize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEIwYX6A3eLq"
      },
      "source": [
        "#main libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from joblib import dump, load\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "%matplotlib inline\n",
        "\n",
        "#model Libraries\n",
        "from sklearn import utils\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#skopt\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFy4LVtlH4Ef"
      },
      "source": [
        "#private class (tentar treinar com a classe da pasta [no colab] a o inves de colar local)\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "#class MyXGBRanker(XGBRanker, BaseEstimator, RegressorMixin):\n",
        "\n",
        "from xgboost import XGBRanker\n",
        "class MyXGBRanker(XGBRanker, BaseEstimator, RegressorMixin):\n",
        "    def fit(self, x, y):\n",
        "        cdf = x.groupby('era').agg(['count'])\n",
        "        group = cdf[cdf.columns[0]].values\n",
        "        return super().fit(x[features], y, group=group)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return super().predict(x[features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FO-dToh2RIp"
      },
      "source": [
        "results = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uUN-aE_08kW"
      },
      "source": [
        "## 2. Download the datasets\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6HHuwUX_xDC"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1HP0LB3nwF6",
        "outputId": "e0a5f9d6-4a5b-4f00-edf7-968f83d0d6ec"
      },
      "source": [
        "%%time\n",
        "df_training,features,target = make_dataset.get_data(nrows=None,\n",
        "                                                    low_memory=False, \n",
        "                                                    dataset=\"training\", \n",
        "                                                    feather=False) #Apenas False colab/aws"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 501808 entries, 0 to 501807\n",
            "Columns: 314 entries, id to target\n",
            "dtypes: float64(311), int32(1), object(2)\n",
            "memory usage: 1.2+ GB\n",
            "None\n",
            "CPU times: user 16.6 s, sys: 2.07 s, total: 18.6 s\n",
            "Wall time: 23 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4pj8f8S_zut"
      },
      "source": [
        "### tournament"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tNtPCrIjvrE",
        "outputId": "444bd3ca-0467-495e-b53e-9156f26c38ee"
      },
      "source": [
        "%%time\n",
        "df_tournament,features,target = make_dataset.get_data(nrows=None,\n",
        "                                                    low_memory=False, \n",
        "                                                    dataset=\"tournament\", \n",
        "                                                    feather=False) #Apenas False colab/aws"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1628412 entries, 0 to 1628411\n",
            "Columns: 314 entries, id to target\n",
            "dtypes: float64(311), object(3)\n",
            "memory usage: 3.8+ GB\n",
            "None\n",
            "CPU times: user 53.9 s, sys: 4.57 s, total: 58.5 s\n",
            "Wall time: 1min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWvGwJSC_3LD"
      },
      "source": [
        "validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDdJMjqFjvhh"
      },
      "source": [
        "df_validation = df_tournament[df_tournament.data_type == 'validation']\n",
        "df_validation['era'] = df_validation.loc[:, 'era'].str[3:].astype('int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F67I2f39amT5"
      },
      "source": [
        "df_tournament=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9w4v5WxSjkN"
      },
      "source": [
        "## 3. Validation Strategy & Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbob23Zl6XJ8"
      },
      "source": [
        "#Group K-fold\n",
        "CV = GroupKFold(n_splits = 3)\n",
        "cv_grp = list(CV.split(X = df_training[features], y = df_training[target],  groups = df_training.era.values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7N1VpEi4OQM"
      },
      "source": [
        "#Group TS-fold\n",
        "CV = group_ts_split.TimeSeriesSplitGroups(n_splits = 5)\n",
        "ts_grp = list(CV.split(X = df_training[features], y = df_training[target],  groups = df_training.era))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqzsSJBTFMmz"
      },
      "source": [
        "#Group Slider-fold\n",
        "CV = group_ts_split.TimeSeriesSplitGroups_Slider(n_splits = 5)\n",
        "slider_grp = list(CV.split(X = df_training[features], y = df_training[target],  groups = df_training.era))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDbQS5NGs4Zr"
      },
      "source": [
        "#Combinatorial K-fold\n",
        "#CV = combinatorial_split.CombinatorialPurgedKFold(n_splits=12, n_test_splits=2, samples_info_sets=combinatorial_split.last_ix_series(df_training))\n",
        "#cpcv_grp = list(CV.split(X = df_training[features], y = df_training['target'],  groups = df_training.era))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Jqydb3QCrs"
      },
      "source": [
        "def eras_score(y_true, y_pred):\n",
        "\n",
        "    #create y_true as df\n",
        "    y_true = y_true.to_frame(name='target')\n",
        "    y_true = y_true.join(df_training['era'])\n",
        "\n",
        "    #create y_pred as df\n",
        "    preds_df = pd.DataFrame(y_pred, index = y_true.index, columns=['preds'])\n",
        "    preds_df = preds_df.join(df_training['era'])\n",
        "    era_scores = pd.Series(index=y_true['era'].unique())\n",
        "\n",
        "    for era in y_true['era'].unique():\n",
        "        era_df = y_true[y_true['era'] == era]\n",
        "        era_preds = preds_df[preds_df['era'] == era]\n",
        "        era_scores[era] = np.corrcoef(era_df['target'], \n",
        "                                      era_preds['preds'].rank(pct=True, method=\"first\"))[0,1]\n",
        "\n",
        "    \n",
        "    results = era_scores.to_frame()\n",
        "    #results.to_csv(file_name_prov, mode='a', header=False, index=True)\n",
        "    return era_scores\n",
        "\n",
        "\n",
        "def validation_mean_(y_true, y_pred):\n",
        "  era_scores = eras_score(y_true, y_pred)\n",
        "  return np.mean(era_scores)\n",
        "\n",
        "\n",
        "def annual_sharpe(x):\n",
        "    return ((np.mean(x) -0.010415154) /np.std(x, ddof=1)) #* np.sqrt(12) \n",
        "\n",
        "from scipy.stats import skew, kurtosis, sem, gmean\n",
        "def adj_sharpe_(y_true, y_pred):\n",
        "  x = eras_score(y_true, y_pred)\n",
        "  return annual_sharpe(x) * (1 + ((skew(x) / 6) * annual_sharpe(x)) - \\\n",
        "                             ((kurtosis(x) - 0) / 24) * (annual_sharpe(x) ** 2)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scorer = make_scorer(adj_sharpe_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALAdBeNj5cj7"
      },
      "source": [
        "## 4. FN Strategy (Pre)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQSQSBT5-6H"
      },
      "source": [
        "### FN Train (metrics)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlOUQWwW5wAv"
      },
      "source": [
        "import scipy\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def _neutralize(df, columns, by, ml_model, proportion=1.0): #['preds'], features,\n",
        "    scores = df[columns] #preds\n",
        "    exposures = df[by].values #features\n",
        "    ml_model[0].fit(exposures, scores.values.reshape(1,-1)[0])\n",
        "    neutr_preds = pd.DataFrame(ml_model[0].predict(exposures), index=df.index, columns=columns)\n",
        "    #exposures.dot(np.linalg.pinv(exposures).dot(scores))    \n",
        "\n",
        "    \n",
        "    if ml_model[1] != None:\n",
        "        ml_model[1].fit(exposures, scores.values.reshape(1,-1)[0])\n",
        "        neutr_preds2 = pd.DataFrame(ml_model[1].predict(exposures), index=df.index, columns=columns)\n",
        "\n",
        "    else: neutr_preds2 = 0# np.zeros(len(scores))\n",
        "\n",
        "    scores = scores - ((proportion * neutr_preds) + ((1-proportion) * neutr_preds2))\n",
        "    #scores = scores - proportion * neutr_preds\n",
        "    return scores / scores.std()\n",
        "\n",
        "\n",
        "\n",
        "def _normalize(df):\n",
        "    X = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return scipy.stats.norm.ppf(X)\n",
        "\n",
        "\n",
        "def normalize_and_neutralize(df, columns, by, ml_model, proportion=1.0):\n",
        "    # Convert the scores to a normal distribution\n",
        "    df[columns] = _normalize(df[columns])\n",
        "    #df[columns] = _neutralize(df, columns, by, ml_model, proportion)\n",
        "    return df[columns]\n",
        "   \n",
        "\n",
        "\n",
        "def preds_neutralized(df, columns, by, ml_model, proportion=1.0):\n",
        "\n",
        "    preds_neutr = df.groupby(\"era\").apply( lambda x: normalize_and_neutralize(x, columns, by, ml_model, proportion))\n",
        "\n",
        "    preds_neutr = MinMaxScaler().fit_transform(preds_neutr).reshape(1,-1)[0]\n",
        "\n",
        "    return preds_neutr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuzWO3M65vz4"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "def adj_sharpe_fn_(y_true, y_pred):\n",
        "\n",
        "  #create y_true as df\n",
        "  y_true_ = y_true.to_frame(name='target')\n",
        "  df_cv = y_true_.join(df_training[['era']+features])\n",
        "  df_cv['preds'] = y_pred\n",
        "\n",
        "  y_pred_fn = preds_neutralized(df_cv, ['preds'], features,\n",
        "                                [LinearRegression(fit_intercept=False), Ridge(alpha=0.5)],\n",
        "                                0.75)\n",
        "                             #fn_strategy['nr__medellin']['model'], \n",
        "                             #fn_strategy['nr__medellin']['factor'])\n",
        "  \n",
        "  x = eras_score(y_true, y_pred_fn)\n",
        "  return annual_sharpe(x) * (1 + ((skew(x) / 6) * annual_sharpe(x)) - \\\n",
        "                             ((kurtosis(x) - 0) / 24) * (annual_sharpe(x) ** 2)) \n",
        "\n",
        "#scorer = make_scorer(adj_sharpe_fn_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2jElgq4-WRE",
        "outputId": "57963b76-691b-4198-c683-27a31cb9e886"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(1000, 4000, name='n_estimators', prior='uniform'), \n",
        "           Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(5e-2, .5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, .5, name='subsample', prior='uniform'),\n",
        "          \n",
        "           #step 5\n",
        "           Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "\n",
        "    #model.set_params(**params)\n",
        "    param_grid = {} #hparams in list []\n",
        "    for key, val in params.items(): param_grid[key] = [val]\n",
        "    print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "    #train model\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=cv_grp, n_jobs=-1, scoring=scorer)\n",
        "    grid_search.fit(df_training[features], df_training[target])\n",
        "    \n",
        "\n",
        "    #get estimator and results\n",
        "    model_ = grid_search.best_estimator_\n",
        "    results = grid_search.cv_results_\n",
        "    adj_sharpe_val = results['mean_test_score'][0]\n",
        "\n",
        "\n",
        "    #predict and neutralize results\n",
        "    df_validation['preds'] = model_.predict(df_validation[features])#.to_list()\n",
        "    preds_test = neutralize.preds_neutralized(df_validation,\n",
        "                                              ['preds'], \n",
        "                                              features,\n",
        "                                              fn_strategy['nr__medellin']['model'], \n",
        "                                              fn_strategy['nr__medellin']['factor'])\n",
        "    \n",
        "\n",
        "    #calculationg eras score\n",
        "    print(\"val: \", metrics_era.validation_mean(df_validation[target], preds_test, df_validation))\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation[target], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "\n",
        "\n",
        "    ##creating csv\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "\n",
        "print(str(scorer))\n",
        "model =XGBRegressor(tree_method = 'gpu_hist',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    n_jobs=-1,\n",
        "                    gpu_id=0)\n",
        "\n",
        "file_name = 'era_scores_xgb_slider20.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "#cols = cols = group_ts_split.results_df_cols(slider_grp, df_training, df_validation)\n",
        "#preds_df =  pd.DataFrame(columns=cols)\n",
        "#preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "#res_gp = dummy_minimize(objective, space, n_calls=60, verbose=1, random_state=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_scorer(adj_sharpe_pre_)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KvMZtdl6Hm9"
      },
      "source": [
        "### FN Pre (Target)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWQ90LsfQ8Rf"
      },
      "source": [
        "def eras_score(y_true, y_pred):\n",
        "    target = 'target_fn'\n",
        "\n",
        "    #create y_true as df\n",
        "    y_true = y_true.to_frame(name=target)\n",
        "    y_true = y_true.join(df_training['era'])\n",
        "\n",
        "    #create y_pred as df\n",
        "    preds_df = pd.DataFrame(y_pred, index = y_true.index, columns=['preds'])\n",
        "    preds_df = preds_df.join(df_training['era'])\n",
        "    era_scores = pd.Series(index=y_true['era'].unique())\n",
        "\n",
        "    for era in y_true['era'].unique():\n",
        "        era_df = y_true[y_true['era'] == era]\n",
        "        era_preds = preds_df[preds_df['era'] == era]\n",
        "        era_scores[era] = np.corrcoef(era_df[target], \n",
        "                                      era_preds['preds'].rank(pct=True, method=\"first\"))[0,1]\n",
        "\n",
        "    \n",
        "    results = era_scores.to_frame()\n",
        "    results.to_csv(file_name_prov, mode='a', header=False, index=True)\n",
        "    return era_scores\n",
        "\n",
        "\n",
        "def validation_mean_(y_true, y_pred):\n",
        "  era_scores = eras_score(y_true, y_pred)\n",
        "  return np.mean(era_scores)\n",
        "\n",
        "\n",
        "def annual_sharpe(x):\n",
        "    return ((np.mean(x) -0.010415154) /np.std(x, ddof=1)) #* np.sqrt(12) \n",
        "\n",
        "from scipy.stats import skew, kurtosis, sem, gmean\n",
        "def adj_sharpe_pre_(y_true, y_pred):\n",
        "  x = eras_score(y_true, y_pred)\n",
        "  return annual_sharpe(x) * (1 + ((skew(x) / 6) * annual_sharpe(x)) - \\\n",
        "                             ((kurtosis(x) - 0) / 24) * (annual_sharpe(x) ** 2)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scorer = make_scorer(adj_sharpe_pre_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvK_nrQp5vhN"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "ml_model = [LinearRegression(fit_intercept=False), None]\n",
        "\n",
        "df_training['target_fn'] = neutralize.preds_neutralized(df_training, \n",
        "                                                        ['target'],\n",
        "                                                        features,\n",
        "                                                        ml_model,\n",
        "                                                        0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "G2UtGc-j6gz5",
        "outputId": "89503ff0-2903-4a31-94fb-c79491e2491f"
      },
      "source": [
        "df_training.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>era</th>\n",
              "      <th>data_type</th>\n",
              "      <th>feature_intelligence1</th>\n",
              "      <th>feature_intelligence2</th>\n",
              "      <th>feature_intelligence3</th>\n",
              "      <th>feature_intelligence4</th>\n",
              "      <th>feature_intelligence5</th>\n",
              "      <th>feature_intelligence6</th>\n",
              "      <th>feature_intelligence7</th>\n",
              "      <th>feature_intelligence8</th>\n",
              "      <th>feature_intelligence9</th>\n",
              "      <th>feature_intelligence10</th>\n",
              "      <th>feature_intelligence11</th>\n",
              "      <th>feature_intelligence12</th>\n",
              "      <th>feature_charisma1</th>\n",
              "      <th>feature_charisma2</th>\n",
              "      <th>feature_charisma3</th>\n",
              "      <th>feature_charisma4</th>\n",
              "      <th>feature_charisma5</th>\n",
              "      <th>feature_charisma6</th>\n",
              "      <th>feature_charisma7</th>\n",
              "      <th>feature_charisma8</th>\n",
              "      <th>feature_charisma9</th>\n",
              "      <th>feature_charisma10</th>\n",
              "      <th>feature_charisma11</th>\n",
              "      <th>feature_charisma12</th>\n",
              "      <th>feature_charisma13</th>\n",
              "      <th>feature_charisma14</th>\n",
              "      <th>feature_charisma15</th>\n",
              "      <th>feature_charisma16</th>\n",
              "      <th>feature_charisma17</th>\n",
              "      <th>feature_charisma18</th>\n",
              "      <th>feature_charisma19</th>\n",
              "      <th>feature_charisma20</th>\n",
              "      <th>feature_charisma21</th>\n",
              "      <th>feature_charisma22</th>\n",
              "      <th>feature_charisma23</th>\n",
              "      <th>feature_charisma24</th>\n",
              "      <th>feature_charisma25</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_wisdom9</th>\n",
              "      <th>feature_wisdom10</th>\n",
              "      <th>feature_wisdom11</th>\n",
              "      <th>feature_wisdom12</th>\n",
              "      <th>feature_wisdom13</th>\n",
              "      <th>feature_wisdom14</th>\n",
              "      <th>feature_wisdom15</th>\n",
              "      <th>feature_wisdom16</th>\n",
              "      <th>feature_wisdom17</th>\n",
              "      <th>feature_wisdom18</th>\n",
              "      <th>feature_wisdom19</th>\n",
              "      <th>feature_wisdom20</th>\n",
              "      <th>feature_wisdom21</th>\n",
              "      <th>feature_wisdom22</th>\n",
              "      <th>feature_wisdom23</th>\n",
              "      <th>feature_wisdom24</th>\n",
              "      <th>feature_wisdom25</th>\n",
              "      <th>feature_wisdom26</th>\n",
              "      <th>feature_wisdom27</th>\n",
              "      <th>feature_wisdom28</th>\n",
              "      <th>feature_wisdom29</th>\n",
              "      <th>feature_wisdom30</th>\n",
              "      <th>feature_wisdom31</th>\n",
              "      <th>feature_wisdom32</th>\n",
              "      <th>feature_wisdom33</th>\n",
              "      <th>feature_wisdom34</th>\n",
              "      <th>feature_wisdom35</th>\n",
              "      <th>feature_wisdom36</th>\n",
              "      <th>feature_wisdom37</th>\n",
              "      <th>feature_wisdom38</th>\n",
              "      <th>feature_wisdom39</th>\n",
              "      <th>feature_wisdom40</th>\n",
              "      <th>feature_wisdom41</th>\n",
              "      <th>feature_wisdom42</th>\n",
              "      <th>feature_wisdom43</th>\n",
              "      <th>feature_wisdom44</th>\n",
              "      <th>feature_wisdom45</th>\n",
              "      <th>feature_wisdom46</th>\n",
              "      <th>target</th>\n",
              "      <th>target_fn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n000315175b67977</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.3874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n0014af834a96cdd</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n001c93979ac41d4</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n0034e4143f22a13</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.3303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n00679d1a636062f</td>\n",
              "      <td>1</td>\n",
              "      <td>train</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>...</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.5992</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 315 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  era data_type  ...  feature_wisdom46  target  target_fn\n",
              "0  n000315175b67977    1     train  ...              0.75    0.50     0.3874\n",
              "1  n0014af834a96cdd    1     train  ...              1.00    0.25     0.3128\n",
              "2  n001c93979ac41d4    1     train  ...              0.75    0.25     0.3431\n",
              "3  n0034e4143f22a13    1     train  ...              1.00    0.25     0.3303\n",
              "4  n00679d1a636062f    1     train  ...              0.75    0.75     0.5992\n",
              "\n",
              "[5 rows x 315 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGraPLhm6gw7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOOYetTM6gr1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD9CDmki6gnk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL4cv7z86gjz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ0-EzsY6gfE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0-1S2Jv6gb5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw3Kv5xt6iDh"
      },
      "source": [
        "### FN Pre (Features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Au08-cMU6f6m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Fcz-PpHYez"
      },
      "source": [
        "## 5. Vanilla Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0rYCeSg_9hN"
      },
      "source": [
        "Estratégia modelo Sao Paulo (colab):\n",
        "\n",
        "Validacao:\n",
        "3-GRP fold\n",
        "random ou skopt?\n",
        "\n",
        "\n",
        "Modelos\n",
        "- Linear Regresion\n",
        "- Orthogonal Matching Pursuit\n",
        "- Ridge Regression (ou Bayesian ridge)\n",
        "- Xgboost\n",
        "- Light GBM\n",
        "- XGB Ranker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Qd-p4rUKcK"
      },
      "source": [
        "### Slider 5-GRP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd8wHUp08Ry2"
      },
      "source": [
        "#### lineares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKEEDthKLFOp"
      },
      "source": [
        "linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipBWBmP2gDaf",
        "outputId": "6f98a399-3734-4aec-d80b-3dcc6d52119f"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr_param_grid =  {\n",
        "    'fit_intercept' : [True, False],\n",
        "    'normalize' : [True, False],\n",
        "\n",
        "}\n",
        "\n",
        "results = []\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                   LinearRegression(), 'lr',\n",
        "                                   lr_param_grid, \n",
        "                                   scorer, \n",
        "                                   n_iter=1, \n",
        "                                   cv_folds=slider_grp, \n",
        "                                   pipeline=None)\n",
        "\n",
        "results.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for lr model, 501808 examples\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   24.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   24.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Elapsed seconds: 35.859\n",
            "Best lr model: LinearRegression(fit_intercept=False, normalize=True)\n",
            "Best lr score (val): 0.9596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c302scsZLjzV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfYs_Ve6Lb2G"
      },
      "source": [
        "Ridge "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKEqAgQpLYqs"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_param_grid =  {\n",
        "    'fit_intercept' : [True, False],\n",
        "    'normalize' : [True, False],\n",
        "    'alpha' : Real(10**-1, 10**1, \"log-uniform\")\n",
        "\n",
        "}\n",
        "\n",
        "print(grp_type)\n",
        "result = evaluation.evaluate_model_skopt(df_training[features], df_training[target],\n",
        "                                          Ridge(), 'ridge',\n",
        "                                          ridge_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_iter=10, \n",
        "                                          cv_folds=slider_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "results.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDWmK6laLlS-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYxHYFOWLWYZ"
      },
      "source": [
        "OMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-IG27TbLg9Y"
      },
      "source": [
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "\n",
        "omp_param_grid =  {\n",
        "    'n_nonzero_coefs' : Integer(1, len(features), 'uniform')\n",
        "\n",
        "}\n",
        "\n",
        "print(grp_type)\n",
        "result = evaluation.evaluate_model_skopt(df_training[features], df_training[target],\n",
        "                                   OrthogonalMatchingPursuit(), 'omp',\n",
        "                                   omp_param_grid, \n",
        "                                   scorer, \n",
        "                                   n_iter=10, \n",
        "                                   cv_folds=slider_grp, \n",
        "                                   pipeline=None)\n",
        "\n",
        "results.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-gL5OEduQnz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxKtaA1r8U4q"
      },
      "source": [
        "#### GBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w1lrgfIHHzy"
      },
      "source": [
        "##### xgboost vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSw2ynV7b0yR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6f5e70-f53b-4545-b48c-41ee8fd677b9"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(1000, 4000, name='n_estimators', prior='uniform'), \n",
        "           Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(5e-2, .5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, .5, name='subsample', prior='uniform'),\n",
        "          \n",
        "           #step 5\n",
        "           Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "\n",
        "    #model.set_params(**params)\n",
        "    param_grid = {} #hparams in list []\n",
        "    for key, val in params.items(): param_grid[key] = [val]\n",
        "    print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "    #train model\n",
        "    #print(model)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=slider_grp, n_jobs=-1, scoring=scorer)\n",
        "    grid_search.fit(df_training[features], df_training[target])\n",
        "    \n",
        "\n",
        "    #get estimator and results\n",
        "    model_ = grid_search.best_estimator_\n",
        "    results = grid_search.cv_results_\n",
        "    adj_sharpe_val = results['mean_test_score'][0]\n",
        "\n",
        "\n",
        "    #predict and neutralize results\n",
        "    preds_test = model_.predict(df_validation[features])#.to_list()\n",
        "    #df_validation['preds'] = model_.predict(df_validation[features])#.to_list()\n",
        "    #preds_test = neutralize.preds_neutralized(df_validation,\n",
        "                                              #['preds'], \n",
        "                                              #features,\n",
        "                                              #fn_strategy['nr__medellin']['model'], \n",
        "                                              #fn_strategy['nr__medellin']['factor'])\n",
        "    \n",
        "\n",
        "    #calculationg eras score\n",
        "    print(\"val: \", metrics_era.validation_mean(df_validation['target'], preds_test, df_validation))\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation['target'], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "\n",
        "\n",
        "    ##creating csv\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "target = 'target_fn'\n",
        "print(str(scorer))\n",
        "print(target)\n",
        "model =XGBRegressor(tree_method = 'gpu_hist',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    n_jobs=-1,\n",
        "                    gpu_id=0)\n",
        "\n",
        "file_name = 'era_scores_xgb_slider20.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols = cols = group_ts_split.results_df_cols(slider_grp, df_training, df_validation)\n",
        "preds_df =  pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=30, verbose=1, random_state=300)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make_scorer(adj_sharpe_pre_)\n",
            "target_fn\n",
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "{'n_estimators': 3529, 'learning_rate': 0.013980786598051327, 'max_depth': 5, 'min_child_weight': 10, 'gamma': 0.00047879101992313357, 'colsample_bytree': 0.15848506737625978, 'subsample': 0.4019208948320882, 'reg_alpha': 41.89756003444102}\n",
            "val:  0.01971690104993401\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 107.6041\n",
            "Function value obtained: 0.2075\n",
            "Current minimum: 0.2075\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "{'n_estimators': 2394, 'learning_rate': 0.058323942446671245, 'max_depth': 6, 'min_child_weight': 4, 'gamma': 0.043148547714635645, 'colsample_bytree': 0.343364394392695, 'subsample': 0.43636599675502347, 'reg_alpha': 14.914389957208082}\n",
            "val:  0.01544465282032878\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 124.3490\n",
            "Function value obtained: -0.1191\n",
            "Current minimum: -0.1191\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "{'n_estimators': 1105, 'learning_rate': 0.05526322020740237, 'max_depth': 10, 'min_child_weight': 5, 'gamma': 0.07161822085922483, 'colsample_bytree': 0.08003643663284246, 'subsample': 0.1271513407205914, 'reg_alpha': 2.5595479496718907}\n",
            "val:  0.013683741327158437\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 61.2022\n",
            "Function value obtained: 0.2274\n",
            "Current minimum: -0.1191\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "{'n_estimators': 3596, 'learning_rate': 0.06275288597372805, 'max_depth': 8, 'min_child_weight': 10, 'gamma': 0.022189188988314318, 'colsample_bytree': 0.2870644526453354, 'subsample': 0.41143786891375744, 'reg_alpha': 22.31808132489137}\n",
            "val:  0.019748047074165314\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 198.1963\n",
            "Function value obtained: -0.0920\n",
            "Current minimum: -0.1191\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "{'n_estimators': 1567, 'learning_rate': 0.019277013251126185, 'max_depth': 5, 'min_child_weight': 7, 'gamma': 0.00020586703494577773, 'colsample_bytree': 0.07456546826370228, 'subsample': 0.3841192905291405, 'reg_alpha': 4.150456820130123}\n",
            "val:  0.02076189352600096\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 94.9610\n",
            "Function value obtained: -0.1370\n",
            "Current minimum: -0.1370\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "{'n_estimators': 3419, 'learning_rate': 0.04632593399690283, 'max_depth': 4, 'min_child_weight': 6, 'gamma': 0.00042922633990289755, 'colsample_bytree': 0.41585225520269165, 'subsample': 0.17464875434358623, 'reg_alpha': 1.3833403838070364}\n",
            "val:  0.010477603385334903\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 150.9612\n",
            "Function value obtained: -0.0447\n",
            "Current minimum: -0.1370\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "{'n_estimators': 1680, 'learning_rate': 0.02591846266859429, 'max_depth': 3, 'min_child_weight': 4, 'gamma': 0.0028414949770237457, 'colsample_bytree': 0.39919049601803186, 'subsample': 0.3445759702200156, 'reg_alpha': 15.238792962340526}\n",
            "val:  0.019196259106100982\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 60.1325\n",
            "Function value obtained: -0.1219\n",
            "Current minimum: -0.1370\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "{'n_estimators': 1687, 'learning_rate': 0.04053373401867522, 'max_depth': 10, 'min_child_weight': 7, 'gamma': 0.12215977076464939, 'colsample_bytree': 0.3984635912178203, 'subsample': 0.20650901421206153, 'reg_alpha': 21.942193347308987}\n",
            "val:  0.015149337041795817\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 37.7298\n",
            "Function value obtained: 0.2677\n",
            "Current minimum: -0.1370\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "{'n_estimators': 1095, 'learning_rate': 0.011344957071833603, 'max_depth': 9, 'min_child_weight': 9, 'gamma': 1.391153886013272, 'colsample_bytree': 0.47768543511641237, 'subsample': 0.3990053098363704, 'reg_alpha': 9.72391725584459}\n",
            "val:  -0.0031148605048096837\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 22.4000\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "{'n_estimators': 1120, 'learning_rate': 0.02116681785551642, 'max_depth': 6, 'min_child_weight': 7, 'gamma': 0.8434581551471, 'colsample_bytree': 0.16042301507303558, 'subsample': 0.4807652160505716, 'reg_alpha': 54.82103223827853}\n",
            "val:  0.006690350102867214\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 21.8734\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 11 started. Evaluating function at random point.\n",
            "{'n_estimators': 3293, 'learning_rate': 0.06792305094356636, 'max_depth': 6, 'min_child_weight': 1, 'gamma': 7.143943066152805e-05, 'colsample_bytree': 0.10506856779864086, 'subsample': 0.08790067133245218, 'reg_alpha': 1.0036337765490022}\n",
            "val:  0.0016813212388102784\n",
            "Iteration No: 11 ended. Evaluation done at random point.\n",
            "Time taken: 277.3678\n",
            "Function value obtained: 0.3942\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 12 started. Evaluating function at random point.\n",
            "{'n_estimators': 1459, 'learning_rate': 0.08525791309043108, 'max_depth': 3, 'min_child_weight': 6, 'gamma': 0.00015294552375652595, 'colsample_bytree': 0.3021027559657341, 'subsample': 0.07949526603681192, 'reg_alpha': 68.55420414876629}\n",
            "val:  0.008796344560887893\n",
            "Iteration No: 12 ended. Evaluation done at random point.\n",
            "Time taken: 27.1524\n",
            "Function value obtained: -79.5472\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 13 started. Evaluating function at random point.\n",
            "{'n_estimators': 2675, 'learning_rate': 0.009805796779895019, 'max_depth': 10, 'min_child_weight': 1, 'gamma': 0.0001354161166831151, 'colsample_bytree': 0.32421755268296437, 'subsample': 0.40802364080482834, 'reg_alpha': 1.9459304724760553}\n",
            "val:  0.016603920400828167\n",
            "Iteration No: 13 ended. Evaluation done at random point.\n",
            "Time taken: 1009.7400\n",
            "Function value obtained: -0.2017\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 14 started. Evaluating function at random point.\n",
            "{'n_estimators': 2247, 'learning_rate': 0.014922162673010999, 'max_depth': 5, 'min_child_weight': 8, 'gamma': 0.034148802411356274, 'colsample_bytree': 0.4292980616414046, 'subsample': 0.38689626300945507, 'reg_alpha': 82.09973804099911}\n",
            "val:  0.013897008094977261\n",
            "Iteration No: 14 ended. Evaluation done at random point.\n",
            "Time taken: 46.8760\n",
            "Function value obtained: 0.3939\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 15 started. Evaluating function at random point.\n",
            "{'n_estimators': 3604, 'learning_rate': 0.016443792166042824, 'max_depth': 7, 'min_child_weight': 3, 'gamma': 2.6865695607117162e-05, 'colsample_bytree': 0.24859780334914056, 'subsample': 0.22039130535420037, 'reg_alpha': 6.906861990111845}\n",
            "val:  0.01781904342385929\n",
            "Iteration No: 15 ended. Evaluation done at random point.\n",
            "Time taken: 334.9971\n",
            "Function value obtained: -0.1922\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 16 started. Evaluating function at random point.\n",
            "{'n_estimators': 2379, 'learning_rate': 0.08065166774729726, 'max_depth': 9, 'min_child_weight': 5, 'gamma': 2.444362969577449, 'colsample_bytree': 0.09360613755900724, 'subsample': 0.31090549815173113, 'reg_alpha': 23.532869082851622}\n",
            "val:  -0.0031148605048096837\n",
            "Iteration No: 16 ended. Evaluation done at random point.\n",
            "Time taken: 37.4995\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 17 started. Evaluating function at random point.\n",
            "{'n_estimators': 3832, 'learning_rate': 0.07973879648140812, 'max_depth': 7, 'min_child_weight': 8, 'gamma': 0.00014433918928397912, 'colsample_bytree': 0.34895731718946316, 'subsample': 0.08136843268835826, 'reg_alpha': 1.303325462215155}\n",
            "val:  0.0052061632025851\n",
            "Iteration No: 17 ended. Evaluation done at random point.\n",
            "Time taken: 462.1317\n",
            "Function value obtained: 0.4715\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 18 started. Evaluating function at random point.\n",
            "{'n_estimators': 1121, 'learning_rate': 0.04065169357741273, 'max_depth': 4, 'min_child_weight': 1, 'gamma': 2.2532380283591398e-05, 'colsample_bytree': 0.35963690839405593, 'subsample': 0.340186338082418, 'reg_alpha': 62.711429257657166}\n",
            "val:  0.017811759959504462\n",
            "Iteration No: 18 ended. Evaluation done at random point.\n",
            "Time taken: 32.6979\n",
            "Function value obtained: 0.3502\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 19 started. Evaluating function at random point.\n",
            "{'n_estimators': 1178, 'learning_rate': 0.054260011590984196, 'max_depth': 6, 'min_child_weight': 5, 'gamma': 0.004774819385711409, 'colsample_bytree': 0.13029133080629346, 'subsample': 0.4897925013817562, 'reg_alpha': 2.965625957356591}\n",
            "val:  0.013191357821174032\n",
            "Iteration No: 19 ended. Evaluation done at random point.\n",
            "Time taken: 97.7543\n",
            "Function value obtained: 0.0032\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 20 started. Evaluating function at random point.\n",
            "{'n_estimators': 3030, 'learning_rate': 0.09584196849384088, 'max_depth': 3, 'min_child_weight': 10, 'gamma': 5.354325111015136e-05, 'colsample_bytree': 0.3795992077452422, 'subsample': 0.07372114900185489, 'reg_alpha': 6.351597488019923}\n",
            "val:  0.01014650362548733\n",
            "Iteration No: 20 ended. Evaluation done at random point.\n",
            "Time taken: 102.2071\n",
            "Function value obtained: 0.1932\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 21 started. Evaluating function at random point.\n",
            "{'n_estimators': 2653, 'learning_rate': 0.09478813957203852, 'max_depth': 5, 'min_child_weight': 4, 'gamma': 1.6773379339961314, 'colsample_bytree': 0.26228560172623255, 'subsample': 0.3129442712596491, 'reg_alpha': 50.534615281444026}\n",
            "val:  -0.0031148605048096837\n",
            "Iteration No: 21 ended. Evaluation done at random point.\n",
            "Time taken: 39.9656\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 22 started. Evaluating function at random point.\n",
            "{'n_estimators': 1365, 'learning_rate': 0.07152741352684769, 'max_depth': 7, 'min_child_weight': 5, 'gamma': 0.00023331945827852207, 'colsample_bytree': 0.38632226717395834, 'subsample': 0.4941723633931678, 'reg_alpha': 26.051108698952948}\n",
            "val:  0.016074499501964233\n",
            "Iteration No: 22 ended. Evaluation done at random point.\n",
            "Time taken: 81.7403\n",
            "Function value obtained: -0.0976\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 23 started. Evaluating function at random point.\n",
            "{'n_estimators': 3445, 'learning_rate': 0.03884856909528627, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 3.3662529994276924e-05, 'colsample_bytree': 0.4487626140650427, 'subsample': 0.22969064709701476, 'reg_alpha': 67.34360605048795}\n",
            "val:  0.017608271560626925\n",
            "Iteration No: 23 ended. Evaluation done at random point.\n",
            "Time taken: 72.9458\n",
            "Function value obtained: 0.4324\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 24 started. Evaluating function at random point.\n",
            "{'n_estimators': 1427, 'learning_rate': 0.03479590711405932, 'max_depth': 7, 'min_child_weight': 6, 'gamma': 2.9723625252270592e-05, 'colsample_bytree': 0.4578023492655325, 'subsample': 0.42540939188595034, 'reg_alpha': 1.2036474689141559}\n",
            "val:  0.014932112041571804\n",
            "Iteration No: 24 ended. Evaluation done at random point.\n",
            "Time taken: 197.9792\n",
            "Function value obtained: -0.0623\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 25 started. Evaluating function at random point.\n",
            "{'n_estimators': 3670, 'learning_rate': 0.04457002641183396, 'max_depth': 7, 'min_child_weight': 5, 'gamma': 0.40144764293730384, 'colsample_bytree': 0.2811923176956599, 'subsample': 0.180709248455173, 'reg_alpha': 4.642600725742104}\n",
            "val:  -0.0031148605048096837\n",
            "Iteration No: 25 ended. Evaluation done at random point.\n",
            "Time taken: 54.0787\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 26 started. Evaluating function at random point.\n",
            "{'n_estimators': 1647, 'learning_rate': 0.05939856850379395, 'max_depth': 6, 'min_child_weight': 2, 'gamma': 0.04004116470551329, 'colsample_bytree': 0.10571687444167258, 'subsample': 0.17385340675807023, 'reg_alpha': 14.617045843323908}\n",
            "val:  0.01307127505982218\n",
            "Iteration No: 26 ended. Evaluation done at random point.\n",
            "Time taken: 64.8418\n",
            "Function value obtained: 0.0367\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 27 started. Evaluating function at random point.\n",
            "{'n_estimators': 3073, 'learning_rate': 0.008227804287722632, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0.40538349447773836, 'colsample_bytree': 0.4941542694822689, 'subsample': 0.2651228739489545, 'reg_alpha': 18.413795942650047}\n",
            "val:  0.012279281679928472\n",
            "Iteration No: 27 ended. Evaluation done at random point.\n",
            "Time taken: 46.4376\n",
            "Function value obtained: -6.6417\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 28 started. Evaluating function at random point.\n",
            "{'n_estimators': 1733, 'learning_rate': 0.09764847778431857, 'max_depth': 4, 'min_child_weight': 9, 'gamma': 2.6128379417715237, 'colsample_bytree': 0.2511076331327024, 'subsample': 0.3144351886460564, 'reg_alpha': 29.94684397575206}\n",
            "val:  -0.0031148605048096837\n",
            "Iteration No: 28 ended. Evaluation done at random point.\n",
            "Time taken: 29.4019\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 29 started. Evaluating function at random point.\n",
            "{'n_estimators': 1890, 'learning_rate': 0.04439733752476259, 'max_depth': 3, 'min_child_weight': 5, 'gamma': 0.07211492304133858, 'colsample_bytree': 0.23529298115875996, 'subsample': 0.472164578891849, 'reg_alpha': 11.048127949525679}\n",
            "val:  0.01896575198723901\n",
            "Iteration No: 29 ended. Evaluation done at random point.\n",
            "Time taken: 48.4485\n",
            "Function value obtained: -0.1551\n",
            "Current minimum: -517.7414\n",
            "Iteration No: 30 started. Evaluating function at random point.\n",
            "{'n_estimators': 3643, 'learning_rate': 0.023679252339536036, 'max_depth': 3, 'min_child_weight': 1, 'gamma': 0.48791374017436107, 'colsample_bytree': 0.16797546409579936, 'subsample': 0.38742776662356554, 'reg_alpha': 18.737635756828197}\n",
            "val:  -0.0031148605048096837\n",
            "Iteration No: 30 ended. Evaluation done at random point.\n",
            "Time taken: 52.9047\n",
            "Function value obtained: -517.7414\n",
            "Current minimum: -517.7414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJ2rMiNRrlE"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/era_scores_xgb_slider20.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0zzwOIlFop",
        "outputId": "ff837e2d-9dd2-4553-dd6c-27b47d26c005"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "#example scripts\n",
        "#XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
        "\n",
        "\n",
        "\n",
        "b={'n_estimators': 2851, \n",
        " 'learning_rate': 0.012559878784975307, \n",
        " 'max_depth': 4, \n",
        " 'min_child_weight': 7, \n",
        " 'gamma': 0.17985836491648036, \n",
        " 'colsample_bytree': 0.2784204086247318, \n",
        " 'subsample': 0.33994697676863767, \n",
        " 'reg_alpha': 2.6691509215708877}\n",
        "\n",
        "\n",
        "xgb_param_grid = {\n",
        "        #step 1 & 6\n",
        "        'n_estimators' : [2851],#Integer(1000, 5000, 'uniform'),\n",
        "        'learning_rate' : [0.012559878784975307],#Real(10**-3, 10**-1, \"log-uniform\"),\n",
        "\n",
        "\n",
        "        #step 2\n",
        "        'max_depth' : [4],#Integer(3, 10, 'uniform'),\n",
        "        'min_child_weight':[7],# Integer(1, 10, 'uniform'),\n",
        "\n",
        "        #step 3\n",
        "        'gamma' : [0.17985836491648036],#Real(1e-5, 6, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #step 4\n",
        "        'subsample' : [0.33994697676863767],#Real(1e-1, 1, 'uniform'),\n",
        "        'colsample_bytree':[0.2784204086247318], #Real(1e-1, .5, 'uniform'), \n",
        "\n",
        "        #step 5\n",
        "        'reg_alpha' : [2.6691509215708877],#Real(1, 100, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #utils\n",
        "        'objective': [\"reg:squarederror\"],\n",
        "        #'early_stopping_rounds' : [30],\n",
        "        'tree_method' : ['gpu_hist'],#['gpu_hist'], \n",
        "        'gpu_id' : [0],\n",
        "        \n",
        "}\n",
        "\n",
        "\n",
        "gpu_params = { 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                          XGBRegressor(), 'xgb_slider20',\n",
        "                                          xgb_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_jobs=-1, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=slider_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "\n",
        "result[0].model.get_booster().set_param(gpu_params)\n",
        "results['xgb_slider20'] = result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for xgb_slider20 model, 501808 examples\n",
            "[1.69657795 0.84173346 0.65021149 0.44486033 0.41062111]\n",
            "==> Elapsed seconds: 238.180\n",
            "Best xgb_slider20 model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.22923619312392746,\n",
            "             gamma=0.009652923007014539, gpu_id=0, importance_type='gain',\n",
            "             interaction_constraints='', learning_rate=0.015105554114971671,\n",
            "             max_delta_step=0, max_depth=9, min_child_weight=1, missing=nan,\n",
            "             monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0...,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
            "             n_estimators=4646, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
            "             reg_alpha=24.748680026879075, reg_lambda=1, scale_pos_weight=1,\n",
            "             subsample=0.32960762834422813, tree_method='gpu_hist',\n",
            "             validate_parameters=1, verbosity=None)\n",
            "Best xgb_slider20 score (val): 0.8088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FULwH6W8LlmF"
      },
      "source": [
        "##### lgbm vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9dUfd-0cQ33"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import cross_validate, GridSearchCV\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1&5\n",
        "           Integer(1000, 4000, name='n_estimators', prior='uniform'), #1000,4000\n",
        "           Real(5**-3, 10**-1,  name='learning_rate', prior='log-uniform'),\n",
        "\n",
        "           #step 2\n",
        "           Integer(50, 250, name='num_leaves', prior='uniform'),\n",
        "           Integer(100, 2000, name='min_data_in_leaf', prior='uniform'),\n",
        "           Integer(3, 30, name='max_depth', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(5e-2, 0.5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, 0.5, name='subsample', prior='uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(10**-2, 1.0, name='reg_lambda', prior='log-uniform'),\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "\n",
        "    #model.set_params(**params)\n",
        "    param_grid = {} #hparams in list []\n",
        "    for key, val in params.items(): param_grid[key] = [val]\n",
        "    print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "    #train model\n",
        "    print(model)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=slider_grp, n_jobs=-1, scoring=scorer)\n",
        "    grid_search.fit(df_training[features], df_training[target])\n",
        "    \n",
        "\n",
        "    #get estimator and results\n",
        "    model_ = grid_search.best_estimator_\n",
        "    results = grid_search.cv_results_\n",
        "    adj_sharpe_val = results['mean_test_score'][0]\n",
        "\n",
        "\n",
        "    #predict and neutralize results\n",
        "    preds_test = model_.predict(df_validation[features])#.to_list()\n",
        "    #df_validation['preds'] = model_.predict(df_validation[features])#.to_list()\n",
        "    #preds_test = neutralize.preds_neutralized(df_validation,\n",
        "                                              #['preds'], \n",
        "                                              #features,\n",
        "                                              #fn_strategy['nr__medellin']['model'], \n",
        "                                              #fn_strategy['nr__medellin']['factor'])\n",
        "    \n",
        "\n",
        "    #calculationg eras score\n",
        "    print(\"val: \", metrics_era.validation_mean(df_validation['target'], preds_test, df_validation))\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation['target'], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "\n",
        "\n",
        "    ##creating csv\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "target = 'target_fn'\n",
        "print(str(scorer))\n",
        "print(target)\n",
        "model =LGBMRegressor(device_type = 'gpu',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    boosting_type= 'goss',\n",
        "                    n_jobs=-1)\n",
        "\n",
        "file_name = 'era_scores_lgbm_slider20.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols =  group_ts_split.results_df_cols(slider_grp, df_training, df_validation)\n",
        "preds_df =  pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=10, verbose=1, random_state=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7WGOaBOOu3vi",
        "outputId": "d726a7e5-f5a2-4cbc-af04-af66ea99680b"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/era_scores_lgbm_slider20.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c5d6aa5b-c976-4b12-ad4c-a2173d0c4531\", \"era_scores_lgbm_slider20.csv\", 30358)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFqJLWV5LRQa"
      },
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "#LGBMRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1, num_leaves=32)\n",
        "\n",
        "\n",
        "\n",
        "lgbm_best={'n_estimators': 2258, 'learning_rate': 0.008203629273160581, \n",
        " 'num_leaves': 92, 'min_data_in_leaf': 1395, 'max_depth': 19, \n",
        " 'colsample_bytree': 0.14430607619531413, \n",
        " 'subsample': 0.17193342288927632, \n",
        " 'reg_lambda': 0.13942748601876273,\n",
        " 'boosting_type': 'goss',\n",
        "  'device_type' : 'gpu'\n",
        " }\n",
        "\n",
        "\n",
        "lgbm_param_grid = {\n",
        "    \n",
        "    #step 1 & 6\n",
        "    'n_estimators' : [2258],#Integer(500, 4000, 'uniform'), #[623]\n",
        "    'learning_rate' :[0.008203629273160581],# Real(10**-3, 10**-1, \"log-uniform\"),  #[0.1]\n",
        "\n",
        "\n",
        "    #step 2\n",
        "    'num_leaves': [92], #Integer(50, 250, \"uniform\"), #\n",
        "    'min_data_in_leaf' : [1395],# Integer(100, 2000, 'uniform'), #\n",
        "    'max_depth' : [19], #Integer(3, 30, 'uniform'), #\n",
        "    \n",
        "\n",
        "    #step 3\n",
        "    #'bagging_fraction' : [0.3195476883574932],#Real(1e-1, 1, 'uniform'), #\n",
        "    #'bagging_temperature': [0.5776416670125987],# Real(1e-1, 1.0, 'uniform'), #\n",
        "    \n",
        "    #step 4\n",
        "    'colsample_bytree': [0.14430607619531413], #Real(1e-1, .5, 'uniform'), #\n",
        "    'subsample': [0.17193342288927632], #Real(1e-1, 1.0, 'uniform'), #\n",
        "\n",
        "    #step 5\n",
        "    'reg_lambda': [0.13942748601876273], #Real(10**-2, 1, \"log-uniform\"), #\n",
        "\n",
        "\n",
        "    #utils\n",
        "    'boosting_type': ['goss'],\n",
        "    'device_type' : ['gpu']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#gpu_params = {'device_type':'cpu'}\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                          LGBMRegressor(), 'lgbm_slider20',\n",
        "                                          lgbm_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=slider_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "\n",
        "#result[0].model.set_params(**gpu_params)\n",
        "results['lgbm_slider20'] = result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJv3Pp_kiAiA"
      },
      "source": [
        "##### XGB Ranker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AVztuHrcT8a"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(2000, 5000, name='n_estimators', prior='uniform'), \n",
        "           Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(5e-2, .5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, .5, name='subsample', prior='uniform'),\n",
        "          \n",
        "           #step 5\n",
        "           Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    #print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "\n",
        "    res_dict = cross_validate(model, \n",
        "                               df_training[features], \n",
        "                               df_training[target], \n",
        "                               groups=df_training.era,\n",
        "                               return_estimator=True,\n",
        "                               #return_train_score=True,\n",
        "                               scoring = scorer, \n",
        "                               n_jobs=-1,\n",
        "                               cv=slider_grp)\n",
        "    \n",
        "\n",
        "    adj_sharpe_val = res_dict['test_score'].mean()\n",
        "    preds_test = res_dict['estimator'][-1].predict(df_validation[features]).tolist()\n",
        "\n",
        "\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation[target], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "    \n",
        "\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "class MyXGBRanker(XGBRanker, BaseEstimator, RegressorMixin):\n",
        "    def fit(self, x, y):\n",
        "        cdf = x.groupby('era').agg(['count'])\n",
        "        group = cdf[cdf.columns[0]].values\n",
        "        return super().fit(x[features], y, group=group)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return super().predict(x[features])\n",
        "\n",
        "\n",
        "print(str(scorer))\n",
        "model =MyXGBRanker(tree_method = 'gpu_hist',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    n_jobs=-1,\n",
        "                    gpu_id=0)\n",
        "\n",
        "file_name = 'era_scores_xgb_ranker_slider5.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols = cols = group_ts_split.results_df_cols(slider_grp, df_training, df_validation)\n",
        "preds_df =  pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=200, verbose=1, random_state=54)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp0PMcPPh_CB"
      },
      "source": [
        "from xgboost import XGBRanker\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "class MyXGBRanker(XGBRanker, BaseEstimator, RegressorMixin):\n",
        "    def fit(self, x, y):\n",
        "        cdf = x.groupby('era').agg(['count'])\n",
        "        group = cdf[cdf.columns[0]].values\n",
        "        return super().fit(x[features], y, group=group)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return super().predict(x[features])\n",
        "\n",
        "\n",
        "xgb_param_grid = {\n",
        "        #step 1 & 6\n",
        "        'n_estimators' : [4547], #Integer(3000, 6000, 'uniform'),\n",
        "        'learning_rate' : [0.01], #Real(10**-3, 10**-1, \"log-uniform\"), \n",
        "\n",
        "\n",
        "        #step 2\n",
        "        'max_depth' : [3], #Integer(3, 10, 'uniform'),\n",
        "        'min_child_weight':[9], #Integer(8, 16, 'uniform'), \n",
        "\n",
        "        #step 3\n",
        "        'gamma' : [0.00003253],# Real(1e-5, 6, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #step 4\n",
        "        'subsample' : [1],# Real(1e-1, 1, 'uniform'),\n",
        "        'colsample_bytree':[0.1553875313199311],# Real(1e-1, .5, 'uniform'), \n",
        "\n",
        "        #step 5\n",
        "        'reg_alpha' : [50.20283749898673], #Real(1, 100, 'log-uniform'),\n",
        "\n",
        "        \n",
        "        #utils\n",
        "        #'early_stopping_rounds' : [30],\n",
        "        'tree_method' : ['gpu_hist'],#['gpu_hist'], \n",
        "        'gpu_id' : [0],\n",
        "\n",
        "        \n",
        "}\n",
        "\n",
        "\n",
        "#gpu_params = {'updater':'', 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "result = evaluation.evaluate_model_skopt(df_training[['era']+features], df_training[target],\n",
        "                                          MyXGBRanker(), 'xgb_ranker',\n",
        "                                          xgb_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_jobs=-1, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=cv_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "#result[0].model.get_booster().set_param(gpu_params)\n",
        "results['xgb_ranker'] = result\n",
        "\n",
        "#step 1: 0.0352\n",
        "#step 2: 0.0460\n",
        "#step 3: 0.0460\n",
        "#step 4: 0.0457\n",
        "#step 5: 0.0465\n",
        "#step 6: 0.0480"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsfaQKp2oF1N"
      },
      "source": [
        "#### Salvando os modelos (vanilla)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAld7Yr7oF1P"
      },
      "source": [
        "model_name = [\n",
        "              #\"lr\"\n",
        "              #,\"ridge\"\n",
        "              #,\"omp\"\n",
        "              #\"xgb_slider20\"\n",
        "              \"lgbm_slider20\"\n",
        "              #\"xgb_ranker\"             \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgPd7FY4oF1S"
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "#Maior media, senao sera ultimo feito, \n",
        "#results = sorted(results, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "\n",
        "for name in model_name:\n",
        "  #model = list(filter(lambda x: x[1] == name, results))[-1][0]\n",
        "  dump(results[name][0], name + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8OVmeUcdwc9"
      },
      "source": [
        "### Expanding 5-GRP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uWF1axWdwc9"
      },
      "source": [
        "#### lineares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRO08VZadwc9"
      },
      "source": [
        "linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jli3Lgmhdwc9",
        "outputId": "6f98a399-3734-4aec-d80b-3dcc6d52119f"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr_param_grid =  {\n",
        "    'fit_intercept' : [True, False],\n",
        "    'normalize' : [True, False],\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                   LinearRegression(), 'lr',\n",
        "                                   lr_param_grid, \n",
        "                                   scorer, \n",
        "                                   n_iter=1, \n",
        "                                   cv_folds=ts_grp, \n",
        "                                   pipeline=None)\n",
        "\n",
        "results.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for lr model, 501808 examples\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   24.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   24.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Elapsed seconds: 35.859\n",
            "Best lr model: LinearRegression(fit_intercept=False, normalize=True)\n",
            "Best lr score (val): 0.9596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXQpszvFdwc-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQVc5exXdwc-"
      },
      "source": [
        "Ridge "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaU6UYm1dwc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a524aa2-2127-42d7-a68d-8e9c680862bc"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_param_grid =  {\n",
        "    'fit_intercept' : [True, False],\n",
        "    'normalize' : [True, False],\n",
        "    'alpha' : Real(10**-1, 10**1, \"log-uniform\")\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                          Ridge(), 'ridge',\n",
        "                                          ridge_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=ts_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "results['ridge'] = result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for ridge model, 501808 examples\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    4.8s remaining:    7.2s\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    7.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Elapsed seconds: 9.304\n",
            "Best ridge model: Ridge(alpha=40)\n",
            "Best ridge score (val): 0.7682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8kETKtidwc-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YTriCXSdwc-"
      },
      "source": [
        "OMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67N0LhDrhGjC"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [    \n",
        "           Categorical([True, False], name='fit_intercept'),\n",
        "           Categorical([True, False], name='normalize'),\n",
        "           Integer(10, len(features), name='n_nonzero_coefs', prior='uniform')\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    #print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "\n",
        "    res_dict = cross_validate(model, \n",
        "                               df_training[features], \n",
        "                               df_training[target], \n",
        "                               groups=df_training.era,\n",
        "                               return_estimator=True,\n",
        "                               #return_train_score=True,\n",
        "                               scoring = scorer, \n",
        "                               n_jobs=-1,\n",
        "                               cv=ts_grp)\n",
        "    \n",
        "\n",
        "    adj_sharpe_val = res_dict['test_score'].mean()\n",
        "    preds_test = res_dict['estimator'][-1].predict(df_validation[features]).tolist()\n",
        "\n",
        "\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation[target], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "    \n",
        "\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "print(str(scorer))\n",
        "model =OrthogonalMatchingPursuit()\n",
        "\n",
        "file_name = 'era_scores_omp_exp20.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols = cols = group_ts_split.results_df_cols(ts_grp, df_training, df_validation)\n",
        "preds_df =  pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=200, verbose=1, random_state=34)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHbxCfAndwc-",
        "outputId": "4ea634c4-317f-475d-f446-16bb7cc76b17"
      },
      "source": [
        "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
        "\n",
        "omp_param_grid =  {\n",
        "    'fit_intercept' : [True],\n",
        "    'normalize' : [False],\n",
        "    'n_nonzero_coefs' : [92],#Integer(1, len(features), 'uniform')\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "result = evaluation.evaluate_model_skopt(df_training[features], df_training[target],\n",
        "                                   OrthogonalMatchingPursuit(), 'omp_exp20',\n",
        "                                   omp_param_grid, \n",
        "                                   scorer, \n",
        "                                   n_iter=1, \n",
        "                                   cv_folds=ts_grp, \n",
        "                                   pipeline=None)\n",
        "\n",
        "\n",
        "results['omp_exp20'] = result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for omp_exp20 model, 501808 examples\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Elapsed seconds: 11.128\n",
            "Best omp_exp20 model: OrthogonalMatchingPursuit(n_nonzero_coefs=92, normalize=False)\n",
            "Best omp_exp20 score (val): 0.7894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1BKeqMhdwc-"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOtojR-dwc_"
      },
      "source": [
        "#### GBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUAIpYWzdwc_"
      },
      "source": [
        "##### xgboost vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm5WIVEDdwc_"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(2000, 5000, name='n_estimators', prior='uniform'), \n",
        "           Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(5e-2, .5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, .5, name='subsample', prior='uniform'),\n",
        "          \n",
        "           #step 5\n",
        "           Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    #print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "\n",
        "    res_dict = cross_validate(model, \n",
        "                               df_training[features], \n",
        "                               df_training[target], \n",
        "                               groups=df_training.era,\n",
        "                               return_estimator=True,\n",
        "                               #return_train_score=True,\n",
        "                               scoring = scorer, \n",
        "                               n_jobs=-1,\n",
        "                               cv=ts_grp)\n",
        "    \n",
        "\n",
        "    adj_sharpe_val = res_dict['test_score'].mean()\n",
        "    preds_test = res_dict['estimator'][-1].predict(df_validation[features]).tolist()\n",
        "\n",
        "\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation[target], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "    \n",
        "\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "print(str(scorer))\n",
        "model =XGBRegressor(tree_method = 'gpu_hist',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    n_jobs=-1,\n",
        "                    gpu_id=0)\n",
        "\n",
        "file_name = 'era_scores_xgb_exp20.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols = group_ts_split.results_df_cols(ts_grp, df_training, df_validation)\n",
        "preds_df =  pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=200, verbose=1, random_state=154)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "62npLpodD57Q",
        "outputId": "6e557246-722b-4f7f-82b7-2add76c67265"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/era_scores_xgb_exp20.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c2c65ac8-de08-4de2-8777-dadb72c23c5b\", \"era_scores_xgb_exp20.csv\", 580635)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1JSqacKdwc_",
        "outputId": "a554a313-2661-4d6d-8cdf-ccb234f4d27c"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "#example scripts\n",
        "#XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
        "\n",
        "\n",
        "\n",
        "b={'n_estimators': 2851, \n",
        " 'learning_rate': 0.012559878784975307, \n",
        " 'max_depth': 4, \n",
        " 'min_child_weight': 7, \n",
        " 'gamma': 0.17985836491648036, \n",
        " 'colsample_bytree': 0.2784204086247318, \n",
        " 'subsample': 0.33994697676863767, \n",
        " 'reg_alpha': 2.6691509215708877}\n",
        "\n",
        "\n",
        "xgb_param_grid = {\n",
        "        #step 1 & 6\n",
        "        'n_estimators' : [2851],#Integer(1000, 5000, 'uniform'),\n",
        "        'learning_rate' : [0.012559878784975307],#Real(10**-3, 10**-1, \"log-uniform\"),\n",
        "\n",
        "\n",
        "        #step 2\n",
        "        'max_depth' : [4],#Integer(3, 10, 'uniform'),\n",
        "        'min_child_weight':[7],# Integer(1, 10, 'uniform'),\n",
        "\n",
        "        #step 3\n",
        "        'gamma' : [0.17985836491648036],#Real(1e-5, 6, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #step 4\n",
        "        'subsample' : [0.33994697676863767],#Real(1e-1, 1, 'uniform'),\n",
        "        'colsample_bytree':[0.2784204086247318], #Real(1e-1, .5, 'uniform'), \n",
        "\n",
        "        #step 5\n",
        "        'reg_alpha' : [2.6691509215708877],#Real(1, 100, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #utils\n",
        "        'objective': [\"reg:squarederror\"],\n",
        "        #'early_stopping_rounds' : [30],\n",
        "        'tree_method' : ['gpu_hist'],#['gpu_hist'], \n",
        "        'gpu_id' : [0],\n",
        "        \n",
        "}\n",
        "\n",
        "\n",
        "gpu_params = { 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                          XGBRegressor(), 'xgb_slider20',\n",
        "                                          xgb_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_jobs=-1, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=ts_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "\n",
        "result[0].model.get_booster().set_param(gpu_params)\n",
        "results['xgb_slider20'] = result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for xgb_slider20 model, 501808 examples\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.2min remaining:  1.8min\n",
            "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==> Elapsed seconds: 132.407\n",
            "Best xgb_slider20 model: XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=0.2784204086247318,\n",
            "             gamma=0.17985836491648036, gpu_id=0, importance_type='gain',\n",
            "             interaction_constraints='', learning_rate=0.012559878784975307,\n",
            "             max_delta_step=0, max_depth=4, min_child_weight=7, missing=nan,\n",
            "             monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0...,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
            "             n_estimators=2851, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
            "             reg_alpha=2.6691509215708877, reg_lambda=1, scale_pos_weight=1,\n",
            "             subsample=0.33994697676863767, tree_method='gpu_hist',\n",
            "             validate_parameters=1, verbosity=None)\n",
            "Best xgb_slider20 score (val): 0.7625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5A8onSqdwc_"
      },
      "source": [
        "##### lgbm vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DresLyjIdwc_"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1&5\n",
        "           Integer(1000, 4000, name='n_estimators', prior='uniform'),\n",
        "           Real(5**-3, 10**-1,  name='learning_rate', prior='log-uniform'),\n",
        "\n",
        "           #step 2\n",
        "           Integer(50, 250, name='num_leaves', prior='uniform'),\n",
        "           Integer(100, 2000, name='min_data_in_leaf', prior='uniform'),\n",
        "           Integer(3, 30, name='max_depth', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(5e-2, 0.5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, 0.5, name='subsample', prior='uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(10**-2, 1.0, name='reg_lambda', prior='log-uniform'),\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    #print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "\n",
        "    res_dict = cross_validate(model, \n",
        "                               df_training[features], \n",
        "                               df_training[target], \n",
        "                               groups=df_training.era,\n",
        "                               return_estimator=True,\n",
        "                               #return_train_score=True,\n",
        "                               scoring = scorer, \n",
        "                               n_jobs=-1,\n",
        "                               cv=ts_grp)\n",
        "    \n",
        "\n",
        "    adj_sharpe_val = res_dict['test_score'].mean()\n",
        "    preds_test = res_dict['estimator'][-1].predict(df_validation[features]).tolist()\n",
        "\n",
        "\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation[target], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "    \n",
        "\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "print(str(scorer))\n",
        "model =LGBMRegressor(device_type = 'gpu',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    n_jobs=-1)\n",
        "\n",
        "file_name = 'era_scores_lgbm_exp5.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols = group_ts_split.results_df_cols(slider_grp, df_training, df_validation)\n",
        "preds_df = pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=200, verbose=1, random_state=54)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "kNv05erpdwc_",
        "outputId": "75ab393d-b226-4f94-87d8-663ea5f0c7da"
      },
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "#LGBMRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1, num_leaves=32)\n",
        "\n",
        "\n",
        "b={'n_estimators': 2012, \n",
        " 'learning_rate': 0.008053288987896146, \n",
        " 'num_leaves': 104, \n",
        " 'min_data_in_leaf': 1877, \n",
        " 'max_depth': 19, \n",
        " 'colsample_bytree': 0.2838163562602001, \n",
        " 'subsample': 0.2265121308916066, \n",
        " 'reg_lambda': 0.4781007325965136}\n",
        "\n",
        "lgbm_param_grid = {\n",
        "    \n",
        "\n",
        "    #step 1 & 6\n",
        "    'n_estimators' : [2012],#Integer(500, 4000, 'uniform'), #[623]\n",
        "    'learning_rate' :[0.008053288987896146],# Real(10**-3, 10**-1, \"log-uniform\"),  #[0.1]\n",
        "\n",
        "\n",
        "    #step 2\n",
        "    'num_leaves': [104], #Integer(50, 250, \"uniform\"), #\n",
        "    'min_data_in_leaf' : [1877],# Integer(100, 2000, 'uniform'), #\n",
        "    'max_depth' : [19], #Integer(3, 30, 'uniform'), #\n",
        "    \n",
        "\n",
        "    #step 3\n",
        "    #'bagging_fraction' : [0.3195476883574932],#Real(1e-1, 1, 'uniform'), #\n",
        "    #'bagging_temperature': [0.5776416670125987],# Real(1e-1, 1.0, 'uniform'), #\n",
        "    \n",
        "    #step 4\n",
        "    'colsample_bytree': [0.2838163562602001], #Real(1e-1, .5, 'uniform'), #\n",
        "    'subsample': [0.2265121308916066], #Real(1e-1, 1.0, 'uniform'), #\n",
        "\n",
        "    #step 5\n",
        "    'reg_lambda': [0.4781007325965136], #Real(10**-2, 1, \"log-uniform\"), #\n",
        "\n",
        "\n",
        "    #utils\n",
        "    'boosting_type': ['goss'],\n",
        "    'device_type' : ['gpu']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "gpu_params = {'device_type':'cpu'}\n",
        "result = evaluation.evaluate_model(df_training[features], df_training[target],\n",
        "                                          LGBMRegressor(), 'lgbm_cv40',\n",
        "                                          lgbm_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=cv_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "\n",
        "result[0].model.set_params(**gpu_params)\n",
        "results['lgbm_cv40'] = result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Starting K-fold cross validation for lgbm_exp20 model, 501808 examples\n",
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ed6610f70501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                           \u001b[0mcv_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslider_grp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                           pipeline=None)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dissertacao/src/models/evaluation.py\u001b[0m in \u001b[0;36mevaluate_model_skopt\u001b[0;34m(features, target, model, name, param_grid, scorer, n_iter, cv_folds, n_jobs, pipeline, fit_params)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model_skopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m#if pipeline is None: pipeline = make_pipeline('passthrough')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mtuned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_tuned_model_skopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuned_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mbest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rank_test_score == 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dissertacao/src/models/train_model.py\u001b[0m in \u001b[0;36mbuild_tuned_model_skopt\u001b[0;34m(name, base_model, X_train, y_train, hparams, scorer, n_iter, cv_folds, n_jobs, pipeline, fit_params)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'==> Starting K-fold cross validation for {} model, {} examples'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTunedModel_Skopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==> Elapsed seconds: {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dissertacao/src/models/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, scorer, n_iter, cv_folds, n_jobs, pipeline, fit_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Run it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, callback)\u001b[0m\n\u001b[1;32m    692\u001b[0m                 optim_result = self._step(\n\u001b[1;32m    693\u001b[0m                     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m                     \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_points_adjusted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m                 )\n\u001b[1;32m    696\u001b[0m                 \u001b[0mn_iter\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, X, y, search_space, optimizer, groups, n_points)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mrefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/searchcv.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             )\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m             for train, test in cv_iter)\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL4Je_GLdwc_"
      },
      "source": [
        "##### XGB Ranker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ggb656dwc_"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize, dummy_minimize\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRanker\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(2000, 5000, name='n_estimators', prior='uniform'), \n",
        "           Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(5e-2, .5, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, .5, name='subsample', prior='uniform'),\n",
        "          \n",
        "           #step 5\n",
        "           Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    #print(params)\n",
        "\n",
        "    #cria csv prov zerado aqui\n",
        "    prov_scores =  pd.DataFrame(columns=['scores'])\n",
        "    prov_scores.to_csv(file_name_prov, header=True, index=0)#df_training.era.unique\n",
        "\n",
        "\n",
        "    res_dict = cross_validate(model, \n",
        "                               df_training[features], \n",
        "                               df_training[target], \n",
        "                               groups=df_training.era,\n",
        "                               return_estimator=True,\n",
        "                               #return_train_score=True,\n",
        "                               scoring = scorer, \n",
        "                               n_jobs=-1,\n",
        "                               cv=ts_grp)\n",
        "    \n",
        "\n",
        "    adj_sharpe_val = res_dict['test_score'].mean()\n",
        "    preds_test = res_dict['estimator'][-1].predict(df_validation[features]).tolist()\n",
        "\n",
        "\n",
        "    eras_score_test = metrics_era.numerai_score(df_validation[target], preds_test, df_validation)\n",
        "    eras_score_cv = pd.read_csv(file_name_prov, index_col=0)\n",
        "    \n",
        "\n",
        "    eras_score_full = pd.concat([eras_score_cv.squeeze(), eras_score_test]).to_frame().T\n",
        "    eras_score_full['hparam'] = '{}'.format(params)\n",
        "\n",
        "\n",
        "    eras_score_full.to_csv(file_name, mode='a', header=False, index=False)\n",
        "    return -adj_sharpe_val\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "print(str(scorer))\n",
        "model =XGBRegressor(tree_method = 'gpu_hist',\n",
        "                    objective='rank:pairwise',\n",
        "                    #n_estimators=2000,\n",
        "                    #learning_rate=0.01,\n",
        "                    #max_depth=5,\n",
        "                    #colsample_bytree=0.1,\n",
        "                    n_jobs=-1,\n",
        "                    gpu_id=0)\n",
        "\n",
        "file_name = 'era_scores_xgb_ranker_exp20.csv'\n",
        "file_name_prov = 'prov_scores.csv'\n",
        "\n",
        "\n",
        "#cria csv principal\n",
        "cols = cols = group_ts_split.results_df_cols(ts_grp, df_training, df_validation)\n",
        "preds_df =  pd.DataFrame(columns=cols)\n",
        "preds_df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "\n",
        "res_gp = dummy_minimize(objective, space, n_calls=200, verbose=1, random_state=54)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iphE7e-Bvp4d"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/era_scores_xgb_ranker_exp20.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AJDSj9rdwc_"
      },
      "source": [
        "from xgboost import XGBRanker\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "class MyXGBRanker(XGBRanker, BaseEstimator, RegressorMixin):\n",
        "    def fit(self, x, y):\n",
        "        cdf = x.groupby('era').agg(['count'])\n",
        "        group = cdf[cdf.columns[0]].values\n",
        "        return super().fit(x[features], y, group=group)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return super().predict(x[features])\n",
        "\n",
        "\n",
        "xgb_param_grid = {\n",
        "        #step 1 & 6\n",
        "        'n_estimators' : [4547], #Integer(3000, 6000, 'uniform'),\n",
        "        'learning_rate' : [0.01], #Real(10**-3, 10**-1, \"log-uniform\"), \n",
        "\n",
        "\n",
        "        #step 2\n",
        "        'max_depth' : [3], #Integer(3, 10, 'uniform'),\n",
        "        'min_child_weight':[9], #Integer(8, 16, 'uniform'), \n",
        "\n",
        "        #step 3\n",
        "        'gamma' : [0.00003253],# Real(1e-5, 6, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #step 4\n",
        "        'subsample' : [1],# Real(1e-1, 1, 'uniform'),\n",
        "        'colsample_bytree':[0.1553875313199311],# Real(1e-1, .5, 'uniform'), \n",
        "\n",
        "        #step 5\n",
        "        'reg_alpha' : [50.20283749898673], #Real(1, 100, 'log-uniform'),\n",
        "\n",
        "        \n",
        "        #utils\n",
        "        #'early_stopping_rounds' : [30],\n",
        "        'tree_method' : ['gpu_hist'],#['gpu_hist'], \n",
        "        'gpu_id' : [0],\n",
        "\n",
        "        \n",
        "}\n",
        "\n",
        "\n",
        "#gpu_params = {'updater':'', 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "result = evaluation.evaluate_model_skopt(df_training[['era']+features], df_training[target],\n",
        "                                          MyXGBRanker(), 'xgb_ranker',\n",
        "                                          xgb_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_jobs=-1, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=ts_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "#result[0].model.get_booster().set_param(gpu_params)\n",
        "results['xgb_ranker'] = result\n",
        "\n",
        "#step 1: 0.0352\n",
        "#step 2: 0.0460\n",
        "#step 3: 0.0460\n",
        "#step 4: 0.0457\n",
        "#step 5: 0.0465\n",
        "#step 6: 0.0480"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyTiRI-Idwc_"
      },
      "source": [
        "#### Salvando os modelos (vanilla)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuQgfyRkdwc_"
      },
      "source": [
        "model_name = [\n",
        "              #\"lr\"\n",
        "              #\"ridge\"\n",
        "              #\"omp_exp20\"\n",
        "              #\"xgb_exp20\"\n",
        "              \"lgbm_exp20\"\n",
        "              #\"xgb_ranker\"             \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPp3XdWMdwc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95e871d1-8b17-44fb-e2ba-6b3167e5ceed"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'xgb_slider20': (<train_model.TunedModel at 0x7f43fd1cd6a0>,\n",
              "  'xgb_slider20',\n",
              "  0.7624803868757556,\n",
              "  0.38353656314999585)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cOIDLnLdwc_"
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "#Maior media, senao sera ultimo feito, \n",
        "#results = sorted(results, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "\n",
        "for name in model_name:\n",
        "  #model = list(filter(lambda x: x[1] == name, results))[-1][0]\n",
        "  dump(results[name][0], name + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrkzoO8PgZ-"
      },
      "source": [
        "## 6. FN Transforms (after)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdsKrK8BqJC0"
      },
      "source": [
        "%%time\n",
        "path = '/content/dissertacao/reports/predicoes_validacao/raw/'\n",
        "models_nr = ['ex_preds', 'ex_FN100', 'nr__rio', 'nr__sao_paulo', 'nr__medellin']\n",
        "\n",
        "\n",
        "preds_nr, feat_corrs_nr = dict(), dict()\n",
        "era_scores_nr, df_metrics_nr = dict(), dict()\n",
        "\n",
        "\n",
        "for model in models_nr[0:]:\n",
        "    \n",
        "    #predicoes val1 & val2\n",
        "    print(\"creating predictions to:\", model)\n",
        "    preds_nr[model]=  pd.read_csv(path+model+'_preds_test.csv', index_col='id').values.reshape(1,-1)[0]\n",
        "\n",
        "    \n",
        "\n",
        "    #salvando as metricas\n",
        "    era_scores_nr[model], df_metrics_nr[model], feat_corrs_nr[model], ex_preds = \\\n",
        "                        metrics.submission_metrics(df_validation, preds_nr[model], model, True)   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#dict to dataframe\n",
        "df_preds_nr = pd.DataFrame.from_dict(preds_nr)\n",
        "df_era_scores_nr = pd.DataFrame.from_dict(era_scores_nr)\n",
        "df_feat_corrs_nr = pd.DataFrame.from_dict(feat_corrs_nr)\n",
        "df_metrics_cons_nr = metrics.metrics_consolidated(df_metrics_nr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "4_rm1XB_yPVF",
        "outputId": "1c3f3cfa-d6ea-4447-edf7-1508cfab408d"
      },
      "source": [
        "#print(\"Menor é melhor:\", min_cols)\n",
        "leaderboard_nr = df_metrics_cons_nr[df_metrics_cons_nr.Categoria.isin([\"Performance\", \"Risk\", \"MMC\"])].loc[:,models_nr[0:]]\n",
        "leaderboard_nr.astype(float).style.apply(visualize.diagnostic_colors).apply(visualize.highlight_max, axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col2{\n",
              "            color:  black;\n",
              "            background-color:  yellow;\n",
              "        }#T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col4,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col4,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col4,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col4,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col4{\n",
              "            color:  black;\n",
              "        }#T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col1{\n",
              "            color:  darkgreen;\n",
              "            background-color:  yellow;\n",
              "        }#T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col4,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col4{\n",
              "            color:  darkgreen;\n",
              "        }#T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col1{\n",
              "            color:  lime;\n",
              "            background-color:  yellow;\n",
              "        }#T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col3,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col0,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col1,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col2,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col3{\n",
              "            color:  red;\n",
              "        }#T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col4,#T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col4{\n",
              "            color:  red;\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ex_preds</th>        <th class=\"col_heading level0 col1\" >ex_FN100</th>        <th class=\"col_heading level0 col2\" >nr__rio</th>        <th class=\"col_heading level0 col3\" >nr__sao_paulo</th>        <th class=\"col_heading level0 col4\" >nr__medellin</th>    </tr>    <tr>        <th class=\"index_name level0\" >Metrica</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >Validation_Sharpe</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col0\" class=\"data row0 col0\" >0.9757</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col1\" class=\"data row0 col1\" >0.9757</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col2\" class=\"data row0 col2\" >0.9500</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col3\" class=\"data row0 col3\" >0.8757</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row0_col4\" class=\"data row0 col4\" >0.9678</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >Validation_Mean</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.0266</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.0266</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.0264</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.0245</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row1_col4\" class=\"data row1 col4\" >0.0256</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >Feat_neutral_mean</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.0215</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.0215</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.0206</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.0183</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row2_col4\" class=\"data row2 col4\" >0.0187</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >Validation_SD</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0.0272</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0.0272</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0.0278</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col3\" class=\"data row3 col3\" >0.0280</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row3_col4\" class=\"data row3 col4\" >0.0264</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >Feat_exp_max</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col0\" class=\"data row4 col0\" >0.2694</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col1\" class=\"data row4 col1\" >0.2694</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col2\" class=\"data row4 col2\" >0.2609</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col3\" class=\"data row4 col3\" >0.2933</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row4_col4\" class=\"data row4 col4\" >0.2653</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >Max_Drawdown</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col0\" class=\"data row5 col0\" >-0.0651</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col1\" class=\"data row5 col1\" >-0.0651</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col2\" class=\"data row5 col2\" >-0.0867</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col3\" class=\"data row5 col3\" >-0.0859</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row5_col4\" class=\"data row5 col4\" >-0.0703</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >corr_plus_mmc_sharpe</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col0\" class=\"data row6 col0\" >0.9757</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col1\" class=\"data row6 col1\" >0.9757</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0.8973</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col3\" class=\"data row6 col3\" >0.7949</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row6_col4\" class=\"data row6 col4\" >0.9121</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >val_mmc_mean</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col0\" class=\"data row7 col0\" >0.0000</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0.0000</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col2\" class=\"data row7 col2\" >0.0017</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col3\" class=\"data row7 col3\" >-0.0008</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row7_col4\" class=\"data row7 col4\" >0.0014</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >corr_with_example_preds</th>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col0\" class=\"data row8 col0\" >1.0000</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col1\" class=\"data row8 col1\" >1.0000</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col2\" class=\"data row8 col2\" >0.9088</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col3\" class=\"data row8 col3\" >0.9631</td>\n",
              "                        <td id=\"T_365030b0_56af_11eb_8bc7_0242ac1c0002row8_col4\" class=\"data row8 col4\" >0.8942</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f39653ced30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AbPa6gxvuIh"
      },
      "source": [
        "### MO model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ETy4vfMtjC"
      },
      "source": [
        "Here is a slightly different take on feature neutralization. Instead of finding a linear model of your predictions and subtracting a proportion of it off, we could instead find a linear model that when subtracted off **reduces your feature exposure below a certain target.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM9P7XKXNvwX"
      },
      "source": [
        "We could **set a target and define a loss function** such that when minimized all exposures will be less than or equal to the minimum of current exposure and the maximum desired exposure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg9EQuqjOxw4"
      },
      "source": [
        " So if some features have an exposure of 0.05, and you set a max exposure of 0.10, the features with the exposure of 0.05 won’t necessarily decrease as they would in the current neutralization code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01OS8ZfvOG1g"
      },
      "source": [
        "This allows you to keep some of the smaller exposures that might be important, while reducing your largest risks. Test it out and let me know what you think! Be warned, it’s not especially fast…"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeQ9VT-JPk79"
      },
      "source": [
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch.nn import Sequential\n",
        "from torch.functional import F\n",
        "\n",
        "\n",
        "\n",
        "def exposures(x, y):\n",
        "    x = x - x.mean(dim=0)\n",
        "    x = x / x.norm(dim=0)\n",
        "    y = y - y.mean(dim=0)\n",
        "    y = y / y.norm(dim=0)\n",
        "    return torch.matmul(x.T, y)\n",
        "\n",
        "\n",
        "\n",
        "def reduce_exposure(prediction, features, max_exp):\n",
        "    # linear model of features that will be used to partially neutralize predictions\n",
        "    lin = Linear(features.shape[1],  1, bias=False)\n",
        "    lin.weight.data.fill_(0.)\n",
        "    model = Sequential(lin)\n",
        "    optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n",
        "\n",
        "    feats = torch.tensor(np.float32(features)-.5)\n",
        "    pred = torch.tensor(np.float32(prediction))\n",
        "    start_exp = exposures(feats, pred[:,None])\n",
        "\n",
        "    # set target exposure for each feature to be <= current exposure\n",
        "    # if current exposure is less than max_exp, or <= max_exp if  \n",
        "    # current exposure is > max_exp\n",
        "    targ_exp = torch.clamp(start_exp, -max_exp, max_exp)\n",
        "\n",
        "    for i in range(100000):#100000\n",
        "        optimizer.zero_grad()\n",
        "        # calculate feature exposures of current linear neutralization\n",
        "        exps = exposures(feats, pred[:,None]-model(feats))\n",
        "\n",
        "        # loss is positive when any exposures exceed their target\n",
        "        loss = (F.relu(F.relu(exps)-F.relu(targ_exp)) + F.relu(F.relu(-exps)-F.relu(-targ_exp))).sum()\n",
        "        #print(loss)\n",
        "        print(f'       loss: {loss:0.7f}', end='\\r')\n",
        "\n",
        "        if loss < 1e-7: #7\n",
        "            #print('11111')\n",
        "            neutralizer = [p.detach().numpy() for p in model.parameters()]\n",
        "            neutralized_pred = pred[:,None]-model(feats)\n",
        "            break\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return neutralized_pred, neutralizer\n",
        "\n",
        "\n",
        "\n",
        "def reduce_all_exposures(df, column, neutralizers=[],\n",
        "                                     normalize=True,\n",
        "                                     gaussianize=True,\n",
        "                                     era_col=\"era\",\n",
        "                                     max_exp=0.05):\n",
        "  \n",
        "    #print(max_exp)\n",
        "    unique_eras = df[era_col].unique()\n",
        "    computed = []\n",
        "    for u in unique_eras:\n",
        "        print(u, '\\r') #print era\n",
        "        df_era = df[df[era_col] == u]\n",
        "        scores = df_era[column].values #preds\n",
        "        exposure_values = df_era[neutralizers].values #features\n",
        "        \n",
        "        if normalize:\n",
        "            scores2 = []\n",
        "            for x in scores.T:\n",
        "                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n",
        "                if gaussianize:\n",
        "                    x = scipy.stats.norm.ppf(x)\n",
        "                scores2.append(x)\n",
        "            scores = np.array(scores2)[0]\n",
        "\n",
        "        scores, neut = reduce_exposure(scores, exposure_values, max_exp)\n",
        "\n",
        "        scores /= scores.std()\n",
        "\n",
        "        computed.append(scores.detach().numpy())\n",
        "\n",
        "    return pd.DataFrame(np.concatenate(computed), columns=column, index=df.index)\n",
        "\n",
        "\n",
        "def neutralize_by_threshold(df, column, neutralizers=[],\n",
        "                                     normalize=True,\n",
        "                                     gaussianize=True,\n",
        "                                     era_col=\"era\",\n",
        "                                     max_exp=0.05):\n",
        "  \n",
        "  \n",
        "  data_rfe = reduce_all_exposures(df, column, neutralizers, \n",
        "                                  normalize, gaussianize, \n",
        "                                  era_col=\"era\", \n",
        "                                  max_exp=0.05)\n",
        "  \n",
        "  df[column] = data_rfe[column]\n",
        "  df[column]  -= df[column] .min()\n",
        "  df[column]  /= df[column] .max()\n",
        "\n",
        "  return df[column]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRIIQq3PyJ5"
      },
      "source": [
        "%%time\n",
        "\n",
        "path = '/content/dissertacao/reports/predicoes_validacao/raw/'\n",
        "models_nr = ['ex_preds', 'nr__rio', 'nr__sao_paulo', 'nr__medellin']\n",
        "\n",
        "preds_nr, feat_corrs_nr = dict(), dict()\n",
        "era_scores_nr, df_metrics_nr = dict(), dict()\n",
        "\n",
        "data = df_validation.copy()\n",
        "#data = data[data.era < 123]\n",
        "\n",
        "for model in models_nr[0:]:\n",
        "    \n",
        "    #predicoes val1 & val2\n",
        "    print(\"creating predictions to:\", model)\n",
        "    data['preds'] =  pd.read_csv(path+model+'_preds_test.csv', index_col='id').values.reshape(1,-1)[0]#[:9231]\n",
        "\n",
        "\n",
        "    preds_nr[model] = neutralize_by_threshold(data, \n",
        "                                              ['preds'],\n",
        "                                              features,\n",
        "                                              True, True, \n",
        "                                              era_col=\"era\",\n",
        "                                              max_exp=0.06).values.reshape(1,-1)[0]\n",
        "      \n",
        "    \n",
        "    #salvando as metricas\n",
        "    era_scores_nr[model], df_metrics_nr[model], feat_corrs_nr[model], ex_preds = \\\n",
        "                        metrics.submission_metrics(data, preds_nr[model], model, True)   \n",
        "\n",
        "\n",
        "\n",
        "#add ex_preds\n",
        "#print(\"creating predictions to: ex_preds\")\n",
        "#preds_nr['ex_preds'] = np.array(ex_preds)\n",
        "#era_scores_nr['ex_preds'], df_metrics_nr['ex_preds'], feat_corrs_nr['ex_preds'], ex_preds = \\\n",
        "#                        metrics.submission_metrics(data, preds_nr['ex_preds'], \"ex_preds\", False)\n",
        "\n",
        "\n",
        "\n",
        "#dict to dataframe\n",
        "df_preds_nr = pd.DataFrame.from_dict(preds_nr)\n",
        "df_era_scores_nr = pd.DataFrame.from_dict(era_scores_nr)\n",
        "df_feat_corrs_nr = pd.DataFrame.from_dict(feat_corrs_nr)\n",
        "df_metrics_cons_nr = metrics.metrics_consolidated(df_metrics_nr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "U3kajZ7bIadl",
        "outputId": "22bbc436-a6c3-491f-ef68-14b71ea50489"
      },
      "source": [
        "#print(\"Menor é melhor:\", min_cols)\n",
        "leaderboard_nr = df_metrics_cons_nr[df_metrics_cons_nr.Categoria.isin([\"Performance\", \"Risk\", \"MMC\",])].loc[:,models_nr[0:]]\n",
        "leaderboard_nr.astype(float).style.apply(visualize.diagnostic_colors).apply(visualize.highlight_max, axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col3,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col3{\n",
              "            color:  darkgreen;\n",
              "        }#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col3,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col3{\n",
              "            color:  lime;\n",
              "            background-color:  yellow;\n",
              "        }#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col3,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col3,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col1{\n",
              "            color:  darkgreen;\n",
              "            background-color:  yellow;\n",
              "        }#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col3,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col3,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col2,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col2{\n",
              "            color:  black;\n",
              "        }#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col0,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col1,#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col2{\n",
              "            color:  lime;\n",
              "        }#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col0{\n",
              "            color:  red;\n",
              "        }#T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col3{\n",
              "            color:  black;\n",
              "            background-color:  yellow;\n",
              "        }</style><table id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ex_preds</th>        <th class=\"col_heading level0 col1\" >nr__rio</th>        <th class=\"col_heading level0 col2\" >nr__sao_paulo</th>        <th class=\"col_heading level0 col3\" >nr__medellin</th>    </tr>    <tr>        <th class=\"index_name level0\" >Metrica</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >Validation_Sharpe</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col0\" class=\"data row0 col0\" >1.2067</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col1\" class=\"data row0 col1\" >1.1966</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col2\" class=\"data row0 col2\" >1.1203</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row0_col3\" class=\"data row0 col3\" >1.2504</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >Validation_Mean</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.0253</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.0248</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.0228</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.0232</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >Feat_neutral_mean</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.0214</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.0209</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.0186</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.0193</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >Validation_SD</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0.0209</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0.0207</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0.0203</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row3_col3\" class=\"data row3 col3\" >0.0186</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >Feat_exp_max</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col0\" class=\"data row4 col0\" >0.0500</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col1\" class=\"data row4 col1\" >0.0500</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col2\" class=\"data row4 col2\" >0.0500</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row4_col3\" class=\"data row4 col3\" >0.0500</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >Max_Drawdown</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col0\" class=\"data row5 col0\" >-0.0464</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col1\" class=\"data row5 col1\" >-0.0498</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col2\" class=\"data row5 col2\" >-0.0527</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row5_col3\" class=\"data row5 col3\" >-0.0324</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >corr_plus_mmc_sharpe</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col0\" class=\"data row6 col0\" >1.2067</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col1\" class=\"data row6 col1\" >1.0507</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0.9197</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row6_col3\" class=\"data row6 col3\" >1.0624</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >val_mmc_mean</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col0\" class=\"data row7 col0\" >0.0000</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0.0054</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col2\" class=\"data row7 col2\" >0.0030</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row7_col3\" class=\"data row7 col3\" >0.0045</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >corr_with_example_preds</th>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col0\" class=\"data row8 col0\" >1.0000</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col1\" class=\"data row8 col1\" >0.6587</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col2\" class=\"data row8 col2\" >0.7026</td>\n",
              "                        <td id=\"T_7a815eec_56e3_11eb_8bc7_0242ac1c0002row8_col3\" class=\"data row8 col3\" >0.6448</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f395fda7128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "RnTJUznZhYWZ",
        "outputId": "07dc42e5-bf81-4f63-c752-043a9f18318d"
      },
      "source": [
        "#print(\"Menor é melhor:\", min_cols)\n",
        "leaderboard_nr = df_metrics_cons_nr[df_metrics_cons_nr.Categoria.isin([\"Performance\", \"Risk\", \"MMC\",])].loc[:,models_nr[:]]\n",
        "leaderboard_nr.astype(float).style.apply(visualize.diagnostic_colors).apply(visualize.highlight_max, axis = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style  type=\"text/css\" >\n",
              "#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col0{\n",
              "            color:  darkgreen;\n",
              "            background-color:  yellow;\n",
              "        }#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col1{\n",
              "            color:  darkgreen;\n",
              "        }#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col2,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col1{\n",
              "            color:  black;\n",
              "        }#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col3,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col1,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col3{\n",
              "            color:  black;\n",
              "            background-color:  yellow;\n",
              "        }#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col0,#T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col2{\n",
              "            color:  red;\n",
              "        }</style><table id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >ex_preds</th>        <th class=\"col_heading level0 col1\" >nr__rio</th>        <th class=\"col_heading level0 col2\" >nr__sao_paulo</th>        <th class=\"col_heading level0 col3\" >nr__medellin</th>    </tr>    <tr>        <th class=\"index_name level0\" >Metrica</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
              "                <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row0\" class=\"row_heading level0 row0\" >Validation_Sharpe</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col0\" class=\"data row0 col0\" >1.1834</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col1\" class=\"data row0 col1\" >1.1354</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col2\" class=\"data row0 col2\" >1.0361</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row0_col3\" class=\"data row0 col3\" >1.1509</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row1\" class=\"row_heading level0 row1\" >Validation_Mean</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col0\" class=\"data row1 col0\" >0.0263</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col1\" class=\"data row1 col1\" >0.0253</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col2\" class=\"data row1 col2\" >0.0230</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row1_col3\" class=\"data row1 col3\" >0.0240</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row2\" class=\"row_heading level0 row2\" >Feat_neutral_mean</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col0\" class=\"data row2 col0\" >0.0213</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col1\" class=\"data row2 col1\" >0.0208</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col2\" class=\"data row2 col2\" >0.0184</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row2_col3\" class=\"data row2 col3\" >0.0192</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row3\" class=\"row_heading level0 row3\" >Validation_SD</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col0\" class=\"data row3 col0\" >0.0222</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col1\" class=\"data row3 col1\" >0.0223</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col2\" class=\"data row3 col2\" >0.0222</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row3_col3\" class=\"data row3 col3\" >0.0209</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row4\" class=\"row_heading level0 row4\" >Feat_exp_max</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col0\" class=\"data row4 col0\" >0.1000</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col1\" class=\"data row4 col1\" >0.1000</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col2\" class=\"data row4 col2\" >0.1000</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row4_col3\" class=\"data row4 col3\" >0.1000</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row5\" class=\"row_heading level0 row5\" >Max_Drawdown</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col0\" class=\"data row5 col0\" >-0.0444</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col1\" class=\"data row5 col1\" >-0.0461</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col2\" class=\"data row5 col2\" >-0.0522</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row5_col3\" class=\"data row5 col3\" >-0.0332</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row6\" class=\"row_heading level0 row6\" >corr_plus_mmc_sharpe</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col0\" class=\"data row6 col0\" >1.1834</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col1\" class=\"data row6 col1\" >1.0465</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col2\" class=\"data row6 col2\" >0.8730</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row6_col3\" class=\"data row6 col3\" >1.0609</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row7\" class=\"row_heading level0 row7\" >val_mmc_mean</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col0\" class=\"data row7 col0\" >0.0000</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col1\" class=\"data row7 col1\" >0.0034</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col2\" class=\"data row7 col2\" >0.0006</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row7_col3\" class=\"data row7 col3\" >0.0028</td>\n",
              "            </tr>\n",
              "            <tr>\n",
              "                        <th id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002level0_row8\" class=\"row_heading level0 row8\" >corr_with_example_preds</th>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col0\" class=\"data row8 col0\" >1.0000</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col1\" class=\"data row8 col1\" >0.7748</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col2\" class=\"data row8 col2\" >0.8248</td>\n",
              "                        <td id=\"T_a8fe1f02_56cb_11eb_8bc7_0242ac1c0002row8_col3\" class=\"data row8 col3\" >0.7572</td>\n",
              "            </tr>\n",
              "    </tbody></table>"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f395fc63ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAxMDVM3ehH"
      },
      "source": [
        "creating csv preds MO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FInTuVLw7Rw7",
        "outputId": "29b7f9ba-fb1b-4099-a254-5043d9ba2ea2"
      },
      "source": [
        "%%time\n",
        "\n",
        "from joblib import dump, load\n",
        "path =''# '../../reports/predicoes_validacao/'\n",
        "\n",
        "models, res_cv, preds = dict(), dict(), dict()\n",
        "#models_nr = ['ex_preds', 'ex_FN100', 'nr__rio', 'nr__sao_paulo', 'nr__medellin']\n",
        "\n",
        "\n",
        "for model in models_nr:\n",
        "    #load model complete pipe\n",
        "    print(\"creating predictions to:\", model)\n",
        "    #models[model] = load(file_path + model + '-cv.pkl')\n",
        "    #preds[model] = models[model].model.predict(df_validation[features])\n",
        "\n",
        "    # predictions must have an `id` column and a `prediction_kazutsugi` column\n",
        "    predictions_df = data[\"id\"].to_frame()\n",
        "    predictions_df[model] = preds_nr[model]\n",
        "    predictions_df.to_csv(path+model +\"_preds_test.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating predictions to: ex_preds\n",
            "creating predictions to: nr__rio\n",
            "creating predictions to: nr__sao_paulo\n",
            "creating predictions to: nr__medellin\n",
            "CPU times: user 1.06 s, sys: 16 ms, total: 1.08 s\n",
            "Wall time: 1.07 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoWCb-H57Rpy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5jS-CG7RjU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkjruqQ_uVVS"
      },
      "source": [
        "## 7. Meta Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir7PEjV_RVhf"
      },
      "source": [
        "### Creating Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6vLG5nnGGjc"
      },
      "source": [
        "Get model files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnTu5Yye81M0"
      },
      "source": [
        "#import glob\n",
        "#from joblib import dump, load\n",
        "#file_path = '/content/dissertacao/models/sao_paulo/'\n",
        "\n",
        "#preds = glob.glob(file_path+'*-cv.pkl')\n",
        "#preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IpVTHxNGI0D"
      },
      "source": [
        "%%time\n",
        "from joblib import dump, load\n",
        "\n",
        "\n",
        "file_path = '/content/dissertacao/models/sao_paulo/models_gpu/'\n",
        "models_meta, preds_meta = dict(), dict()\n",
        "\n",
        "\n",
        "model_cv = ['lr', 'ridge', 'omp', 'xgb', 'lgbm', 'xgb_ranker']\n",
        "model_ts = ['lr_ts', 'ridge_ts', 'omp_ts', 'xgb_ts', 'lgbm_ts', 'xgb_ranker_ts']\n",
        "model_forest = ['xgb_forest', \"lgbm_forest\", \"xgb_ranker_forest\"]\n",
        "\n",
        "\n",
        "model_names_meta = model_cv + model_ts + model_forest\n",
        "#model_names_meta = ['omp', 'lr']#, 'xgb_ranker_forest']\n",
        "\n",
        "\n",
        "for model in model_names_meta:\n",
        "    \n",
        "    #load model complete pipe\n",
        "    #print(\"loading model:\", model)\n",
        "    models_meta[model] = load(file_path + model + '-cv.pkl')\n",
        "\n",
        "#stack_data = meta_model.create_preds_meta_full(models_meta, df_training, splits=3)\n",
        "#stack_data = meta_model.create_preds_meta_light(models_meta, df_training, splits=3)\n",
        "\n",
        "\n",
        "#import pickle\n",
        "#with open('stacked_data.pickle', 'wb') as handle: #write\n",
        "#    pickle.dump(stack_data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSegtjY1D2uO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8NMyC8nDQLH"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpPaUfLGy59X"
      },
      "source": [
        "Get L1-Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "VX8-oBK4y457",
        "outputId": "a4b6ad97-0a85-45e2-8c0d-8d1c88dd8b32"
      },
      "source": [
        "%%time\n",
        "import meta_model\n",
        "from joblib import dump, load\n",
        "\n",
        "file_path = '/content/dissertacao/models/sao_paulo/models_gpu/'\n",
        "models_meta, preds_meta = dict(), dict()\n",
        "\n",
        "\n",
        "model_cv = ['lr', 'ridge', 'omp', 'xgb', 'lgbm', 'xgb_ranker']\n",
        "model_ts = ['lr_ts', 'ridge_ts', 'omp_ts', 'xgb_ts', 'lgbm_ts', 'xgb_ranker_ts']\n",
        "model_forest = ['xgb_forest', \"lgbm_forest\", \"xgb_ranker_forest\"]\n",
        "model_names_meta = model_cv + model_ts + model_forest\n",
        "\n",
        "#get data\n",
        "df_train_l1, l1_features = meta_model.get_stacked_data(meta_model=\"Sao_Paulo\", local=\"colab\")\n",
        "#df_train_l1, l1_features = \\\n",
        "#              meta_model.mount_stacked_data_light(stacked_data, model_names_meta, df_training)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.72 s, sys: 73.7 ms, total: 1.8 s\n",
            "Wall time: 3.02 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjt-qRwQMipA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0cOVQD1D9RH"
      },
      "source": [
        "### Meta Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIHrYNpwO_wy"
      },
      "source": [
        "#### Ensemble GRP-Kfold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oVLNfY3O_w3"
      },
      "source": [
        "#Group K-fold\n",
        "CV = GroupKFold(n_splits = 6)\n",
        "cv_grp = list(CV.split(X = df_training[features], y = df_training[target],  groups = df_training.era.values))\n",
        "\n",
        "#Validacao usada\n",
        "grp_type=\"group_cv\" #group_cv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GBb84lhsmJh"
      },
      "source": [
        "\n",
        "Metricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVxLdGE7smJl"
      },
      "source": [
        "def spearman(target, pred):\n",
        "    from scipy import stats\n",
        "    return stats.spearmanr(target, pred)[0]\n",
        "\n",
        "scorer = make_scorer(spearman)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqLIunvwsmJp"
      },
      "source": [
        "Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LjWDxUqsmJp"
      },
      "source": [
        "results_meta = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdXPISFdvTqe"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ30ApT4iI85"
      },
      "source": [
        "Ridge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxeki_u_sXUX"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge_param_grid =  {\n",
        "    'fit_intercept' : [True, False],\n",
        "    'normalize' : [True, False],\n",
        "    'alpha' : Real(10**-1, 10**1, \"log-uniform\")\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "result = evaluation.evaluate_model_skopt(df_train_l1[l1_features], \n",
        "                                         df_train_l1['target_nomi'],\n",
        "                                         Ridge(), 'ridge_l1',\n",
        "                                         ridge_param_grid, \n",
        "                                         scorer, \n",
        "                                         n_iter=10, \n",
        "                                         cv_folds=cv_grp, \n",
        "                                         pipeline=None)\n",
        "\n",
        "results_meta['ridge_l1'] = result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVyUlxNGwN62"
      },
      "source": [
        "pd.DataFrame(result[0].model.coef_, index=l1_features,columns=[\"coef_l1\"]).sort_values('coef_l1', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOQo20QZq7Sq"
      },
      "source": [
        "from joblib import dump, load\n",
        "dump(results_meta['ridge_l1'][0], 'ridge_l1' + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc5omvzB--uI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUNHu2Hf_AeR"
      },
      "source": [
        "LGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egkwxAIe-xoj"
      },
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "#LGBMRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1, num_leaves=32)\n",
        "\n",
        "lgbm_param_grid = {\n",
        "    \n",
        "\n",
        "    #step 1 & 6\n",
        "    'n_estimators' : [3870],#Integer(1500, 5000, 'uniform'), \n",
        "    'learning_rate' : [0.0011020213071919218], #Real(10**-3, 10**-1, \"log-uniform\"),\n",
        "\n",
        "\n",
        "    #step 2\n",
        "    'num_leaves': [249], #Integer(50, 250, \"uniform\"), #\n",
        "    'min_data_in_leaf' :[1948],# Integer(100, 2000, 'uniform'), #\n",
        "    'max_depth' : [3],#Integer(3, 30, 'uniform'), #\n",
        "    \n",
        "\n",
        "    #step 3\n",
        "    'bagging_fraction' : [0.1384607156999449],#Real(1e-1, 1, 'uniform'), #\n",
        "    'bagging_temperature': [0.22109455109869675],# Real(1e-1, 1.0, 'uniform'), #\n",
        "    \n",
        "    #step 4 l1 no usar\n",
        "    #'colsample_bytree': Real(1e-1, .5, 'uniform'), # l1 nao usar\n",
        "    #'subsample': [1] #Real(1e-1, 1.0, 'uniform'), # l1 nao usar\n",
        "\n",
        "    #step 5\n",
        "    'reg_lambda': [0.19530125277968313], #Real(10**-2, 1, \"log-uniform\"), #\n",
        "\n",
        "\n",
        "    #utils\n",
        "    'boosting_type': ['goss'],\n",
        "    'device_type' : ['gpu']\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "print(grp_type)\n",
        "#gpu_params = {'device_type':'cpu'}\n",
        "result = evaluation.evaluate_model_skopt(df_train_l1[l1_features], \n",
        "                                         df_train_l1['target_nomi'],\n",
        "                                          LGBMRegressor(), 'lgbm_l1',\n",
        "                                          lgbm_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=cv_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "\n",
        "#result[0].model.set_params(**gpu_params)\n",
        "results_meta['lgbm_l1'] = result\n",
        "\n",
        "\n",
        "#score step 1: 0.0xx\n",
        "#score step 2: 0.0365\n",
        "#score step 3: 0.0373\n",
        "#score step 4: 0.0xx\n",
        "#score step 5: 0.0369\n",
        "#score step 6: 0.0478"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le_G9E4qq4zu"
      },
      "source": [
        "from joblib import dump, load\n",
        "dump(results_meta['lgbm_l1'][0], 'lgbm_l1' + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9xOEIGPsV37"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XtEvkw6PJG4"
      },
      "source": [
        "#### TS-GRP fold validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "X-Kqzz-xPJG5",
        "outputId": "9d6f31aa-5583-4c0b-b324-da6a94af67fd"
      },
      "source": [
        "#Group TS-fold\n",
        "ts_grp = group_ts_split.TimeSeriesSplitGroups(6, df_training.era)\n",
        "\n",
        "#Validacao usada\n",
        "grp_type=\"time_series\" \n",
        "print(ts_grp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TimeSeriesSplitGroups(grp=None, n_splits=6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEO0E-vRNUvb"
      },
      "source": [
        "\n",
        "Metricas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWuzk8iXNUvg"
      },
      "source": [
        "def spearman(target, pred):\n",
        "    from scipy import stats\n",
        "    return stats.spearmanr(target, pred)[0]\n",
        "\n",
        "scorer = make_scorer(spearman)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcbPUI87NUvk"
      },
      "source": [
        "Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_JvRHLbNUvl"
      },
      "source": [
        "results_meta = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUpUMvewiyt0"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "#example scripts\n",
        "#XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, n_jobs=-1, colsample_bytree=0.1)\n",
        "\n",
        "xgb_param_grid = {\n",
        "        #step 1 & 6\n",
        "        'n_estimators' : [3010],#Integer(1000, 5000, 'uniform'),\n",
        "        'learning_rate' : [0.021470082387096502],#Real(10**-3, 10**-1, \"log-uniform\"),\n",
        "\n",
        "\n",
        "        #step 2\n",
        "        'max_depth' : [3],#Integer(3, 10, 'uniform'),\n",
        "        'min_child_weight':[1],# Integer(1, 10, 'uniform'),\n",
        "\n",
        "        #step 3\n",
        "        'gamma' : [0.08626899377510583],#Real(1e-5, 6, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #step 4\n",
        "        'subsample' : [0.9983396500245366],#Real(1e-1, 1, 'uniform'),\n",
        "        'colsample_bytree':[0.49725039069087584], #Real(1e-1, .5, 'uniform'), \n",
        "\n",
        "        #step 5\n",
        "        'reg_alpha' : [2.794697948644621],#Real(1, 100, 'log-uniform'),\n",
        "\n",
        "\n",
        "        #utils\n",
        "        'objective': [\"reg:squarederror\"],\n",
        "        #'early_stopping_rounds' : [30],\n",
        "        'tree_method' : ['hist'],#['gpu_hist'], \n",
        "        #'gpu_id' : [0],\n",
        "        \n",
        "}\n",
        "\n",
        "\n",
        "#gpu_params = {'updater':'', 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "result = evaluation.evaluate_model_skopt(df_train_l1[l1_features], \n",
        "                                         df_train_l1['target_nomi'],\n",
        "                                          XGBRegressor(), 'xgb_ts_l1',\n",
        "                                          xgb_param_grid, \n",
        "                                          scorer, \n",
        "                                          n_jobs=-1, \n",
        "                                          n_iter=1, \n",
        "                                          cv_folds=ts_grp, \n",
        "                                          pipeline=None)\n",
        "\n",
        "\n",
        "#result[0].model.get_booster().set_param(gpu_params)\n",
        "results_meta['xgb_ts_l1'] = result\n",
        "\n",
        "#score step 1: 0.0\n",
        "#score step 2: 0.0\n",
        "#score step 3: 0.0\n",
        "#score step 4: 0.0\n",
        "#score step 5: 0.0\n",
        "#score step 6: 0.0401"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_HM-RqMrB9e"
      },
      "source": [
        "from joblib import dump, load\n",
        "dump(results_meta['xgb_ts_l1'][0], 'xgb_ts_l1' + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaBgJfHX6MaU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZY1Xs2QSXr3"
      },
      "source": [
        "#### Block K-fold (forest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A02gUayX-z5T"
      },
      "source": [
        "def spearman(target, pred):\n",
        "    from scipy import stats\n",
        "    return stats.spearmanr(target, pred)[0]\n",
        "results_meta = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpwVjPmXW_aT"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1&5\n",
        "           Integer(1000, 1010, name='n_estimators', prior='uniform'),\n",
        "           #Real(5**-3, 10**-1,  name='learning_rate', prior='log-uniform'),\n",
        "\n",
        "           #step 2\n",
        "           #Integer(50, 250, name='num_leaves', prior='uniform'),\n",
        "           #Integer(100, 2000, name='min_data_in_leaf', prior='uniform'),\n",
        "           #Integer(3, 30, name='max_depth', prior='uniform'),\n",
        "\n",
        "           #step 3 nao usar no l1\n",
        "           #Real(5e-2, 0.5, name='colsample_bytree', prior='uniform'),\n",
        "           #Real(5e-2, 0.5, name='subsample', prior='uniform'),\n",
        "\n",
        "           #step 4\n",
        "           #Real(10**-2, 1.0, name='reg_lambda', prior='log-uniform'),\n",
        "]\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    print(params)\n",
        "\n",
        "    data['block'] = np.trunc(data['era']/20).astype(int)\n",
        "    data.loc[data['block'] == 6, 'block'] = 5\n",
        "\n",
        "    results_val=[]\n",
        "    for block in range(6):\n",
        "        #print(\"Validation Block {}\".format(block)) \n",
        "\n",
        "        train = data[data['block'] != block]\n",
        "        val = data[data['block'] == block]\n",
        "\n",
        "        X_train = train.filter(regex=r'feature')\n",
        "        X_val = val.filter(regex=r'feature')\n",
        "\n",
        "        y_train = train['target_nomi']\n",
        "        y_val = val['target_nomi']\n",
        "        \n",
        "\n",
        "        model_.train(X_train[l1_features], y_train)\n",
        "        preds_val_df = pd.Series(model.predict(X_val[l1_features]), index=X_val.index)\n",
        "\n",
        "\n",
        "        correlation = spearman(y_val, preds_val_df)\n",
        "        results_val.append(correlation)\n",
        "        print(\"Correlation  {}\".format(correlation))\n",
        "\n",
        "    #save best score\n",
        "    model_.save_cv_scores(results_val)\n",
        "    return -np.mean(results_val)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "#gpu_params = {'device_type':'cpu'}\n",
        "model= LGBMRegressor(boosting_type='goss', \n",
        "                     device_type = \"gpu\",\n",
        "                     n_estimators=1002,\n",
        "                     learning_rate=0.0085646,\n",
        "                     num_leaves=250,\n",
        "                     min_data_in_leaf=2000,\n",
        "                     max_depth=3,\n",
        "                     colsample_bytree = 1, #l1\n",
        "                     subsample = 1, #l1\n",
        "                     reg_lambda=0.30464128088558723,\n",
        "                     n_jobs=-1)\n",
        "\n",
        "\n",
        "param = model.get_params()\n",
        "model_ = train_model.Model('lgbm_forest_l1', model, n_iter=1, cv_folds=1,\n",
        "                            n_jobs = -1, pipeline=None, fit_params=param)\n",
        "  \n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "data = df_train_l1.copy()\n",
        "res_gp = forest_minimize(objective, space, n_calls=1, n_random_starts=1, verbose=1, random_state=123)\n",
        "#model.set_params(**gpu_params)\n",
        "results_meta['lgbm_forest_l1'] = model_\n",
        "\n",
        "##Lembre de salvar os cv scores certos\n",
        "#model_.save_cv_scores(results_val)\n",
        "\n",
        "\n",
        "#score step 1: 0.0xx\n",
        "#score step 2: 0.0350\n",
        "#score step 3: 0.0xx\n",
        "#score step 4: 0.0xx\n",
        "#score step 5: 0.0401\n",
        "#score step 6: 0.0480"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxlOs2sGvGtd"
      },
      "source": [
        "model_.results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fkyAJQzhfy7"
      },
      "source": [
        "print(\"fez os 30 com sucesso\")\n",
        "print(res_gp.func_vals)\n",
        "print(res_gp.x_iters)\n",
        "print(res_gp.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "COS9kpuy8UqW",
        "outputId": "d0a9d9cb-de38-4c7a-ddad-af36db3cf645"
      },
      "source": [
        "from joblib import dump, load\n",
        "dump(results_meta['lgbm_forest_l1'], 'lgbm_forest_l1' + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lgbm_forest_l1-cv.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o-jS91b-Fmg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5aUTJ3-L9w"
      },
      "source": [
        "#### Loop aninhado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GUPhQbVjhKu"
      },
      "source": [
        "#get data\n",
        "stacked_data = meta_model.get_stacked_data_full(meta_model=\"Sao_Paulo\", local=\"colab\")\n",
        "\n",
        "def spearman(target, pred):\n",
        "    from scipy import stats\n",
        "    return stats.spearmanr(target, pred)[0]\n",
        "\n",
        "    \n",
        "results_meta = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOEfGI1hW_GG"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(500, 3000, name='n_estimators', prior='uniform'), \n",
        "           #Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           #Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           #Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           #Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4 l1 nao usar\n",
        "           #Real(5e-2, .5, name='colsample_bytree', prior='uniform'),\n",
        "           #Real(5e-2, 1.0, name='subsample', prior='uniform'),\n",
        "          \n",
        "           #step 5\n",
        "           #Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    print(params)\n",
        "\n",
        "    results_val=[]\n",
        "    for fold in data.keys():\n",
        "        #print(\"Training on {}\".format(fold))\n",
        "\n",
        "        X_train = data[fold]['Xtrain']\n",
        "        X_val = data[fold]['Xtest']\n",
        "\n",
        "        y_train = data[fold]['ytrain']\n",
        "        y_val = data[fold]['ytest']\n",
        "\n",
        "\n",
        "        model_.train(X_train, y_train)\n",
        "        correlation = spearman(y_val, model.predict(X_val))\n",
        "        results_val.append(correlation)\n",
        "\n",
        "        print(\"Correlation  {}\".format(correlation))\n",
        "        #print()\n",
        "\n",
        "    #save best score\n",
        "    model_.save_cv_scores(results_val)\n",
        "    return -np.mean(results_val)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "#gpu_params = {'updater':'', 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "model =XGBRegressor(tree_method = \"hist\",\n",
        "                    learning_rate = 0.0515946341276062,\n",
        "                    n_estimators=4280,\n",
        "                    max_depth=3,\n",
        "                    min_child_weight=5,\n",
        "                    gamma =0.13169329887735526,\n",
        "                    colsample_bytree=1,\n",
        "                    subsample=1,\n",
        "                    reg_alpha=7.193526575307788,\n",
        "                    n_jobs=-1)\n",
        "                    #gpu_id=0)\n",
        "\n",
        "\n",
        "param = model.get_params()\n",
        "model_ = train_model.Model('xgb_loop_l1', model, n_jobs=-1, cv_folds=1, \n",
        "                           n_iter=1, pipeline=None, fit_params=param)\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "data = stacked_data.copy()\n",
        "res_gp = forest_minimize(objective, space, n_calls=1, n_random_starts=1, verbose=1, random_state=123)\n",
        "#model.set_params(**gpu_params)\n",
        "results_meta['xgb_loop_l1'] = model_\n",
        "\n",
        "##Lembre de salvar os cv scores certos\n",
        "#model_.save_cv_scores(results_val)\n",
        "\n",
        "#step 1: 0.0475"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C1oYt3iupfo"
      },
      "source": [
        "print(\"fez os 30 com sucesso\")\n",
        "print(res_gp.func_vals)\n",
        "print(res_gp.x_iters)\n",
        "print(res_gp.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGc0K3tB8QJT"
      },
      "source": [
        "from joblib import dump, load\n",
        "dump(results_meta['xgb_loop_l1'], 'xgb_loop_l1' + '-cv.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gya9P_f4-VOo"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngj2YXGh-yoa"
      },
      "source": [
        "## 8. Verificando Modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-42JaTo-5Mk"
      },
      "source": [
        "import glob\n",
        "#from joblib import dump, load\n",
        "#file_path = '/content/dissertacao/models/sao_paulo/'\n",
        "file_path = '/content/dissertacao/models/sao_paulo/models_gpu/'\n",
        "\n",
        "files_path_list = glob.glob(file_path+'*-cv.pkl')\n",
        "\n",
        "\n",
        "\n",
        "for file_ in files_path_list:\n",
        "    \n",
        "    #load model complete pipe\n",
        "    #print(\"creating predictions to:\", model)\n",
        "    models_meta[file_] = load(file_)#(file_path + model + '-cv.pkl') #-120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "kWv9HJX3-5JC",
        "outputId": "a6a48670-c815-40eb-b795-4efce277f3b3"
      },
      "source": [
        "models_meta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/dissertacao/models/sao_paulo/models_gpu/lgbm-cv.pkl': <train_model.TunedModel_Skopt at 0x7f866793c978>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/lgbm_forest-cv.pkl': <train_model.Model at 0x7f8619b6b1d0>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/lgbm_forest_l1-cv.pkl': <train_model.Model at 0x7f8619bc1828>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/lgbm_l1-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619bf7ef0>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/lgbm_ts-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619c0dac8>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/lr-cv.pkl': <train_model.TunedModel at 0x7f8619a73cc0>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/lr_ts-cv.pkl': <train_model.TunedModel at 0x7f8619bf7710>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/omp-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619c1e550>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/omp_ts-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619b5c860>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/ridge-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619a46550>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/ridge_l1-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619b5c908>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/ridge_ts-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619a73748>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619bf8f98>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_forest-cv.pkl': <train_model.Model at 0x7f8619bf84a8>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_loop_l1-cv.pkl': <train_model.Model at 0x7f8619c1eac8>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_ranker-cv.pkl': <train_model.TunedModel_Skopt at 0x7f866793ce48>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_ranker_forest-cv.pkl': <train_model.Model at 0x7f8619bf0a20>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_ranker_ts-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619bbd6a0>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_ts-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619b3ef60>,\n",
              " '/content/dissertacao/models/sao_paulo/models_gpu/xgb_ts_l1-cv.pkl': <train_model.TunedModel_Skopt at 0x7f8619b1a780>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "QDaXdsVAD2-e",
        "outputId": "d932a33f-99eb-49b6-f3d6-aaf3c00aa332"
      },
      "source": [
        "len(files_path_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRjI9wmXKB43"
      },
      "source": [
        "## 9. Special Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2ARDonjRc27"
      },
      "source": [
        "Split Data 1/3 eras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gkfi4mGRc2-"
      },
      "source": [
        "eras_val = df_training.era.unique()[::3]\n",
        "\n",
        "df_special_train = df_training[~df_training.era.isin(eras_val)]\n",
        "df_special_val = df_training[df_training.era.isin(eras_val)]\n",
        "\n",
        "eras_val_ind = df_special_val.era"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U20egIPRc3E"
      },
      "source": [
        "Metrica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2dzAWbDRc3F"
      },
      "source": [
        "def numerai_score(y_true, y_pred):\n",
        "    rank_pred = y_pred.groupby(eras_val_ind).apply(lambda x: x.rank(pct=True, method=\"first\"))\n",
        "    return np.corrcoef(y_true, rank_pred)[0,1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUK94sXbRc3I"
      },
      "source": [
        "Save Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coNZ67xvRc3I"
      },
      "source": [
        "special_models = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXAWW7JUS_J3"
      },
      "source": [
        "#### Xgboost_Sharpe_Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yUANf-Xg7v8"
      },
      "source": [
        "\n",
        "target = \"target_nomi\" \n",
        "path = \"/content/dissertacao/models/sao_paulo/\"\n",
        "name = \"lgbm\"\n",
        "\n",
        "# fit an initial model \"lgbm_vanilla\"\n",
        "model_base = load(path + name + '-cv.pkl') #deveria ser 120\n",
        "\n",
        "\n",
        "#model_base.fit(df_training[features], df_training[target]) # se necessário\n",
        "base_margin = model_base.model.predict(df_special_train[features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsyXlSowQsZA"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "\n",
        "\n",
        "# get indexes for each era\n",
        "era_idx = [np.where(df_special_train.era==uera)[0] for uera in df_special_train.era.unique()]\n",
        "\n",
        "# use correlation as the measure of fit\n",
        "def corr(pred, target):\n",
        "    pred_n = pred - pred.mean(dim=0)\n",
        "    pred_n = pred_n / pred_n.norm(dim=0)\n",
        "\n",
        "    target_n = target - target.mean(dim=0)\n",
        "    target_n = target_n / target_n.norm(dim=0)\n",
        "    l = torch.matmul(pred_n.T, target_n)\n",
        "    return l\n",
        "\n",
        "\n",
        "# definte a custom objective for XGBoost\n",
        "def adj_sharpe_obj(ytrue, ypred):\n",
        "    # convert to pytorch tensors\n",
        "    ypred_th = torch.tensor(ypred, requires_grad=True)\n",
        "    ytrue_th = torch.tensor(ytrue)\n",
        "    all_corrs = []\n",
        "\n",
        "    # get correlations in each era\n",
        "    for ee in era_idx:\n",
        "        score = corr(ypred_th[ee], ytrue_th[ee])\n",
        "        all_corrs.append(score)\n",
        "\n",
        "    all_corrs = torch.stack(all_corrs)\n",
        "\n",
        "    # calculate adjusted sharpe using correlations\n",
        "    loss = -metrics.adj_sharpe(all_corrs)\n",
        "    print(f'Current loss:{loss}')\n",
        "\n",
        "    # calculate gradient and convert to numpy\n",
        "    loss_grads = grad(loss, ypred_th, create_graph=True)[0]\n",
        "    loss_grads = loss_grads.detach().numpy()\n",
        "\n",
        "    # return gradient and ones instead of Hessian diagonal\n",
        "    return loss_grads, np.ones(loss_grads.shape)\n",
        "\n",
        "\n",
        "#model_adj_sharpe = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=200, nthread=6, colsample_bytree=0.1, objective=adj_sharpe_obj)\n",
        "#model_adj_sharpe.fit(df_training[feature_columns], df_training[target], base_margin=base_margin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4mozh1v9EOg"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           Integer(10, 12, name='n_estimators', prior='uniform'), \n",
        "           #Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           #Integer(3, 30, name='max_depth', prior='uniform'),\n",
        "           #Integer(1, 30, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           #Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           #Real(5e-2, 1.0, name='colsample_bytree', prior='uniform'),\n",
        "           #Real(5e-2, 1.0, name='subsample', prior='uniform'),\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    print(params)\n",
        "\n",
        "\n",
        "    model.fit(df_special_train[features], df_special_train[target],  base_margin=base_margin)\n",
        "    preds_df = pd.Series(model.predict(df_special_val[features]), index=df_special_val.index)\n",
        "    \n",
        "\n",
        "    return -numerai_score(df_special_val[target], preds_df)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "gpu_params = {'updater':'', 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "model = XGBRegressor(tree_method = \"gpu_hist\",\n",
        "                     objective=adj_sharpe_obj,\n",
        "                     learning_rate = 0.1,\n",
        "                     #n_estimators= 727,\n",
        "                     #max_depth=4,\n",
        "                     #min_child_weight=7,\n",
        "                     #gamma = 5.7794,\n",
        "                     #colsample_bytree=0.2258,\n",
        "                     #subsample=0.9349,\n",
        "                     #reg_alpha=72.96,\n",
        "                     n_jobs=-1,\n",
        "                     gpu_id=0)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "\n",
        "special_models['xgb_sharpe'] = model\n",
        "res_gp = forest_minimize(objective, space, n_calls=1, n_random_starts=1, verbose=1, random_state=6)\n",
        "model.get_booster().set_param(gpu_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUJsb6qhSusI"
      },
      "source": [
        "print(\"fez os 30 com sucesso\")\n",
        "print(res_gp.func_vals)\n",
        "print(res_gp.x_iters)\n",
        "print(res_gp.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVQZFF8_ENG"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbyo0BT5CHvF"
      },
      "source": [
        "#### Era boosted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DpQNWFyCJlI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0pZz03lCJ8_"
      },
      "source": [
        "def ar1(x):\n",
        "    return np.corrcoef(x[:-1], x[1:])[0,1]\n",
        "\n",
        "def autocorr_penalty(x):\n",
        "    n = len(x)\n",
        "    p = ar1(x)\n",
        "    return np.sqrt(1 + 2*np.sum([((n - i)/n)*p**i for i in range(1,n)]))\n",
        "\n",
        "def smart_sharpe(x):\n",
        "    return np.mean(x)/(np.std(x, ddof=1)*autocorr_penalty(x))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def era_boost_train(X, y, era_col, proportion=0.5, trees_per_step=10, num_iters=200):\n",
        "    \n",
        "    \n",
        "    model = GradientBoostingRegressor(max_depth=5, learning_rate=0.01, \n",
        "                                      max_features=\"sqrt\", subsample=0.5, \n",
        "                                      n_estimators=trees_per_step, \n",
        "                                      warm_start=(num_iters>1))\n",
        "    \n",
        "    features = X.columns\n",
        "    model.fit(X, y)\n",
        "    new_df = X.copy()\n",
        "    new_df[\"target\"] = y\n",
        "    new_df[\"era\"] = era_col\n",
        "    \n",
        "    \n",
        "    for i in range(num_iters-1):\n",
        "        print(f\"iteration {i}\")\n",
        "        # score each era\n",
        "        print(\"predicting on train\")\n",
        "        preds = model.predict(X)\n",
        "        new_df[\"pred\"] = preds\n",
        "        era_scores = pd.Series(index=new_df[\"era\"].unique())\n",
        "        \n",
        "        print(\"getting per era scores\")\n",
        "        for era in new_df[\"era\"].unique():\n",
        "            era_df = new_df[new_df[\"era\"] == era]\n",
        "            era_scores[era] = spearmanr(era_df[\"pred\"], era_df[\"target\"])[0]\n",
        "            \n",
        "            \n",
        "        era_scores.sort_values(inplace=True)\n",
        "        worst_eras = era_scores[era_scores <= era_scores.quantile(proportion)].index\n",
        "        print(list(worst_eras))\n",
        "        worst_df = new_df[new_df[\"era\"].isin(worst_eras)]\n",
        "        era_scores.sort_index(inplace=True)\n",
        "        era_scores.plot(kind=\"bar\")\n",
        "        print(\"performance over time\")\n",
        "        plt.show()\n",
        "        print(\"autocorrelation\")\n",
        "        print(ar1(era_scores))\n",
        "        print(\"mean correlation\")\n",
        "        print(np.mean(era_scores))\n",
        "        print(\"sharpe\")\n",
        "        print(np.mean(era_scores)/np.std(era_scores))\n",
        "        print(\"smart sharpe\")\n",
        "        print(smart_sharpe(era_scores))\n",
        "        model.n_estimators += trees_per_step\n",
        "        print(\"fitting on worst eras\")\n",
        "        model.fit(worst_df[features], worst_df[\"target\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "boost_model = era_boost_train(train_features, train_targets[\"target_kazutsugi\"], \n",
        "                              era_col=train_targets[\"era\"], proportion=0.5, \n",
        "                              trees_per_step=10, num_iters=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzb7SE8uCJ9G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-he_dwHmCJ9J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caF50qoNCJ9M"
      },
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "def spearmanr(target, pred):\n",
        "    return np.corrcoef(\n",
        "        target,\n",
        "        pred.rank(pct=True, method=\"first\")\n",
        "    )[0, 1]\n",
        "\n",
        "def era_boost_train(X, y, era_col, proportion=0.5, trees_per_step=10, num_iters=200):\n",
        "    \n",
        "    \n",
        "    model = XGBRegressor(max_depth=5, learning_rate=0.01, \n",
        "                         n_estimators=trees_per_step, \n",
        "                         n_jobs=-1, colsample_bytree=0.1)\n",
        "    \n",
        "    \n",
        "    features = X.columns\n",
        "    model.fit(X, y)\n",
        "    new_df = X.copy()\n",
        "    new_df[\"target\"] = y\n",
        "    new_df[\"era\"] = era_col\n",
        "    for i in range(num_iters-1):\n",
        "        print(f\"iteration {i}\")\n",
        "        # score each era\n",
        "        print(\"predicting on train\")\n",
        "        preds = model.predict(X)\n",
        "        new_df[\"pred\"] = preds\n",
        "        era_scores = pd.Series(index=new_df[\"era\"].unique())\n",
        "        print(\"getting per era scores\")\n",
        "        for era in new_df[\"era\"].unique():\n",
        "            era_df = new_df[new_df[\"era\"] == era]\n",
        "            era_scores[era] = spearmanr(era_df[\"pred\"], era_df[\"target\"])\n",
        "        era_scores.sort_values(inplace=True)\n",
        "        worst_eras = era_scores[era_scores <= era_scores.quantile(proportion)].index\n",
        "        print(list(worst_eras))\n",
        "        worst_df = new_df[new_df[\"era\"].isin(worst_eras)]\n",
        "        era_scores.sort_index(inplace=True)\n",
        "        era_scores.plot(kind=\"bar\")\n",
        "        print(\"performance over time\")\n",
        "        plt.show()\n",
        "        print(\"autocorrelation\")\n",
        "        print(ar1(era_scores))\n",
        "        print(\"mean correlation\")\n",
        "        print(np.mean(era_scores))\n",
        "        print(\"sharpe\")\n",
        "        print(np.mean(era_scores)/np.std(era_scores))\n",
        "        print(\"smart sharpe\")\n",
        "        print(smart_sharpe(era_scores))\n",
        "        model.n_estimators += trees_per_step\n",
        "        booster = model.get_booster()\n",
        "        print(\"fitting on worst eras\")\n",
        "        model.fit(worst_df[features], worst_df[\"target\"], xgb_model=booster)\n",
        "    return model\n",
        "\n",
        "boost_model = era_boost_train(train_features, train_targets[\"target_kazutsugi\"], \n",
        "                              era_col=train_targets[\"era\"], proportion=0.5, \n",
        "                              trees_per_step=10, num_iters=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8GbjkPdCJ9P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxKYCNUOfuj0"
      },
      "source": [
        "#### Ranker_Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuUjHlagfuj0"
      },
      "source": [
        "#group 1/3 dataset\n",
        "cdf = df_special_train.groupby('era').agg(['count'])\n",
        "group_special = cdf[cdf.columns[0]].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm2V742pfuj1"
      },
      "source": [
        "from skopt.utils import use_named_args\n",
        "from skopt import gp_minimize, forest_minimize, gbrt_minimize\n",
        "from xgboost import XGBRanker\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "space  = [\n",
        "           #step 1 & 6\n",
        "           #Integer(2000, 5000, name='n_estimators', prior='uniform'), \n",
        "           #Real(5**-3, 10**-1, name='learning_rate', prior='log-uniform'), \n",
        "\n",
        "           #step 2\n",
        "           Integer(3, 10, name='max_depth', prior='uniform'),\n",
        "           Integer(1, 10, name='min_child_weight', prior='uniform'),\n",
        "\n",
        "           #step 3\n",
        "           #Real(1e-5, 6, name='gamma', prior='log-uniform'),\n",
        "\n",
        "           #step 4\n",
        "           Real(5e-2, 1.0, name='colsample_bytree', prior='uniform'),\n",
        "           Real(5e-2, 1.0, name='subsample', prior='uniform'),\n",
        "          \n",
        "          #step 5\n",
        "          #Real(1, 100, name='reg_alpha', prior='log-uniform'),\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "@use_named_args(space)\n",
        "def objective(**params):\n",
        "    model.set_params(**params)\n",
        "    print(params)\n",
        "\n",
        "    model.fit(df_special_train[features], df_special_train[target], group=group_special)\n",
        "    preds_df = pd.Series(model.predict(df_special_val[features]), index=df_special_val.index)\n",
        "    \n",
        "    return -numerai_score(df_special_val[target], preds_df)\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "gpu_params = {'updater':'', 'predictor':'cpu_predictor', 'tree_method':'hist'}\n",
        "model = XGBRanker(tree_method = \"gpu_hist\",\n",
        "                  learning_rate = 0.038900611770001495,\n",
        "                  n_estimators= 4731,\n",
        "                  max_depth=4,\n",
        "                  min_child_weight=8,\n",
        "                  gamma = 0.3285961066329761,\n",
        "                  colsample_bytree=0.13505449410675752,\n",
        "                  subsample=0.6468152889357408,\n",
        "                  reg_alpha=47.56020058180222,\n",
        "                  n_jobs=-1,\n",
        "                  gpu_id=0)\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "res_gp = forest_minimize(objective, space, n_calls=30, n_random_starts=10, verbose=1, random_state=23)\n",
        "model.get_booster().set_param(gpu_params)\n",
        "forest_models['xgb_ranker'] = model\n",
        "\n",
        "\n",
        "\n",
        "#step 1: 0.0289\n",
        "#step 2: 0.0371\n",
        "#step 3: 0.0371\n",
        "#step 4: 0.0371\n",
        "#step 5: 0.0371\n",
        "#step 6: 0.0383/0.0391/0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE5HhpWmfuj1"
      },
      "source": [
        "print(\"fez os 30 com sucesso\")\n",
        "print(res_gp.func_vals)\n",
        "print(res_gp.x_iters)\n",
        "print(res_gp.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmlU9ZpIfuj1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}